{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
<<<<<<< HEAD
    "\n",
=======
    "import csv\n",
>>>>>>> parent of 9d9eea6... College crawl update
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl(object):\n",
    "    gap_Insecond=0\n",
    "    max_Pages=10\n",
    "    \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: www.university.edu\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then everth page \n",
    "        respectrobottxt: True\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, _prioritykeywords, _respectrobottxt=True):\n",
    "        self.college=_collegename\n",
    "        self.rootUrl=_rooturl\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "        self.allUrls=[]\n",
    "        self.visitedUrls=[]\n",
    "        self.rejectedUrls=[]\n",
    "        self.n_allUrls=0\n",
    "        self.n_sitedUrls=0\n",
    "        self.n_rejectedUrls=0\n",
    "    \n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def all_pages(self, base_url, links_only=True):\n",
    "        response=requests.get(base_url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "        unique_urls={base_url}\n",
    "        visited_urls=set()\n",
    "        unables=set()\n",
    "        while len(unique_urls)>len(visited_urls):\n",
    "            soup=BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                try:\n",
    "                    url=link['href']                \n",
    "                    parsed_uri = urlparse(url )\n",
    "                    if parsed_uri.netloc=='':\n",
    "                        absolute_url=base_url+url    \n",
    "                    elif parsed_uri.netloc==base_domain:\n",
    "                        absolute_url=url\n",
    "                    else:\n",
    "                        continue\n",
    "                    clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url)\n",
    "                    clean=''.join(clean[0:11])+(''.join(clean[11:])).replace('//', '/')\n",
    "                    clean=re.sub(r'[/\\s]$','', clean)\n",
    "                    unique_urls.add(clean)           \n",
    "                except:\n",
    "                    continue            \n",
    "            unvisited_url=(unique_urls-visited_urls-unables).pop()\n",
    "            visited_urls.add(unvisited_url)\n",
    "            response=requests.get(unvisited_url)\n",
    "            if links_only!=True and response.status_code== 200: #use a different routine to get text data from resonse. \n",
    "                handleOnePage(unvisited_url)\n",
    "            else:\n",
    "                unables.add(unvisited_url)  \n",
    "        return unique_urls\n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def read_OneUrl(self, url):\n",
    "        response=requests.get(url)  \n",
    "        if response.status_code==200: \n",
    "            return self.get_pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "            \n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "\n",
    "    def get_pagetext(self, body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "        \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    \n",
    "    def save_OnePage(self,url,folder=None, filename=None, format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "        if folder==None:\n",
    "            folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder doesnot exist')\n",
    "            return\n",
    "        if filename==None:\n",
    "            filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
<<<<<<< HEAD
    "        #return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def save_Summaries(self):\n",
    "        print(self)\n",
    "        print(\"Total number of URLs visisted: \", len(self.visitedUrls))\n",
    "        print(\"Total number of URLs rejected: \", len(self.rejectedUrls))\n",
    "        print(\"URLs visited (ranked with priority words):\\n\",self.allRankedUrls)\n",
    "    \n",
    "    \"\"\"\n",
    "        Ranks the urls based on the priority words\n",
    "    \"\"\"\n",
    "    def getRankedVisitedUrls(self):\n",
    "        self.allRankedUrls = sorted(self.visitedUrls,key=lambda url: [w in url for w in self.priorityKeywords].count(True),reverse=True)\n",
    "        return self.allRankedUrls\n",
    "    \n",
    "    def setMaxpage(self, maxPages):\n",
    "        CollegeCrawl.max_Pages = maxPages\n",
    "        \n",
    "    def setGap(self, gap):\n",
    "        CollegeCrawl.gap_Insecond = gap\n",
    "    \n",
    "    \n",
    "    def crawl(self):\n",
    "        self.getAllUrls()\n",
    "        rankedUrlList = self.getRankedVisitedUrls()\n",
    "        for url in rankedUrlList:\n",
    "            self.save_OnePage(url)\n",
    "            \n",
    "        self.save_Summaries()"
=======
    "        return fullname           \n",
    "    \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['UCLA','Yale','University of Washington','Stanford']\n",
    "urls=['http://www.ucla.edu/','http://yale.edu/','http://washington.edu/','http://stanford.edu/']\n",
    "k=['apply,admission']\n",
    "for q in range(len(names)):\n",
    "    c=CollegeCrawl(names[q],urls[q],k)\n",
    "    a=c.all_pages(urls[q])\n",
    "    for x in a:\n",
    "        c.save_OnePage(x)"
>>>>>>> parent of 9d9eea6... College crawl update
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "colleges = [[\"UCLA\", \"http://www.ucla.edu\", [\"apply\",\"admission\",\"research\"]],\n",
    "           [\"Stanford\", \"https://www.stanford.edu/\", [\"apply\",\"admission\",\"research\"]],\n",
    "           [\"Yale\", \"https://www.yale.edu/\", [\"apply\",\"admission\",\"research\"]],\n",
    "           [\"UW\", \"http://www.washington.edu/\", [\"apply\",\"admission\",\"research\"]]]\n",
    "for college in colleges:\n",
    "    cr = CollegeCrawl(college[0], college[1], college[2])\n",
    "    cr.crawl()"
   ]
=======
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers sent for URL http://www.mit.edu is {'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
      "response status code for URL http://www.mit.edu is 200\n",
      "Help on function get in module requests.api:\n",
      "\n",
      "get(url, params=None, **kwargs)\n",
      "    Sends a GET request.\n",
      "    \n",
      "    :param url: URL for the new :class:`Request` object.\n",
      "    :param params: (optional) Dictionary, list of tuples or bytes to send\n",
      "        in the query string for the :class:`Request`.\n",
      "    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n",
      "    :return: :class:`Response <Response>` object\n",
      "    :rtype: requests.Response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#response=requests.get(\"https://stanford.edu\", None, headers={'User-Agent': \"*\"})\n",
    "#response=requests.get(\"https://stanford.edu\", headers={'User-Agent':'Mozilla/5.0'})\n",
    "url = \"http://www.mit.edu\"\n",
    "response = requests.get(url)\n",
    "print(\"headers sent for URL {} is {}\".format(url, response.request.headers))\n",
    "print(\"response status code for URL {} is {}\".format(url, response.status_code))\n",
    "help(requests.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> parent of 9d9eea6... College crawl update
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
