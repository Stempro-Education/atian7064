{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl(object):\n",
    "    gap_Insecond=0\n",
    "    max_Pages=10\n",
    "    \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: www.university.edu\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then everth page \n",
    "        respectrobottxt: True\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, _prioritykeywords, _respectrobottxt=True):\n",
    "        self.college=_collegename\n",
    "        self.rootUrl=_rooturl\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "        self.allUrls=[]\n",
    "        self.visitedUrls=[]\n",
    "        self.rejectedUrls=[]\n",
    "        self.n_allUrls=0\n",
    "        self.n_sitedUrls=0\n",
    "        self.n_rejectedUrls=0\n",
    "    \n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def all_pages(self, base_url, links_only=True):\n",
    "        response=requests.get(base_url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "        unique_urls={base_url}\n",
    "        visited_urls=set()\n",
    "        unables=set()\n",
    "        while len(unique_urls)>len(visited_urls):\n",
    "            soup=BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                try:\n",
    "                    url=link['href']                \n",
    "                    parsed_uri = urlparse(url )\n",
    "                    if parsed_uri.netloc=='':\n",
    "                        absolute_url=base_url+url    \n",
    "                    elif parsed_uri.netloc==base_domain:\n",
    "                        absolute_url=url\n",
    "                    else:\n",
    "                        continue\n",
    "                    clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url)\n",
    "                    clean=''.join(clean[0:11])+(''.join(clean[11:])).replace('//', '/')\n",
    "                    clean=re.sub(r'[/\\s]$','', clean)\n",
    "                    unique_urls.add(clean)           \n",
    "                except:\n",
    "                    continue            \n",
    "            unvisited_url=(unique_urls-visited_urls-unables).pop()\n",
    "            visited_urls.add(unvisited_url)\n",
    "            response=requests.get(unvisited_url)\n",
    "            if links_only!=True and response.status_code== 200: #use a different routine to get text data from resonse. \n",
    "                handleOnePage(unvisited_url)\n",
    "            else:\n",
    "                unables.add(unvisited_url)  \n",
    "        return unique_urls\n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def read_OneUrl(self, url):\n",
    "        response=requests.get(url)  \n",
    "        if response.status_code==200: \n",
    "            return self.get_pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "            \n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "\n",
    "    def get_pagetext(self, body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "        \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    \n",
    "    def save_OnePage(self,url,folder=None, filename=None, format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "        if folder==None:\n",
    "            folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder doesnot exist')\n",
    "            return\n",
    "        if filename==None:\n",
    "            filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
    "        return fullname           \n",
    "    \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['UCLA','Yale','University of Washington','Stanford']\n",
    "urls=['http://www.ucla.edu/','http://yale.edu/','http://washington.edu/','http://stanford.edu/']\n",
    "k=['apply,admission']\n",
    "for q in range(len(names)):\n",
    "    c=CollegeCrawl(names[q],urls[q],k)\n",
    "    a=c.all_pages(urls[q])\n",
    "    for x in a:\n",
    "        c.save_OnePage(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers sent for URL http://www.mit.edu is {'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
      "response status code for URL http://www.mit.edu is 200\n",
      "Help on function get in module requests.api:\n",
      "\n",
      "get(url, params=None, **kwargs)\n",
      "    Sends a GET request.\n",
      "    \n",
      "    :param url: URL for the new :class:`Request` object.\n",
      "    :param params: (optional) Dictionary, list of tuples or bytes to send\n",
      "        in the query string for the :class:`Request`.\n",
      "    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n",
      "    :return: :class:`Response <Response>` object\n",
      "    :rtype: requests.Response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#response=requests.get(\"https://stanford.edu\", None, headers={'User-Agent': \"*\"})\n",
    "#response=requests.get(\"https://stanford.edu\", headers={'User-Agent':'Mozilla/5.0'})\n",
    "url = \"http://www.mit.edu\"\n",
    "response = requests.get(url)\n",
    "print(\"headers sent for URL {} is {}\".format(url, response.request.headers))\n",
    "print(\"response status code for URL {} is {}\".format(url, response.status_code))\n",
    "help(requests.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
