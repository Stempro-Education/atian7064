{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urlparse\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl:\n",
    "    gap_Insecond=5\n",
    "    max_Pages=5\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,_collegename, _rooturl, \n",
    "            _prioritykeywords=['apply', 'admission', 'application', 'deadline'], \n",
    "           # _url_file=None,            \n",
    "            _save_to_folder=None,\n",
    "            _respectrobottxt=True):\n",
    "    \n",
    "        self.college=_collegename\n",
    "         \n",
    "        if urlparse(_rooturl).scheme==\"\":\n",
    "            print('URL needs to have scheme. Please try again.')\n",
    "            raise Exception('URL needs to have scheme')            \n",
    "            return\n",
    "        self.rootUrl=_rooturl\n",
    "        \n",
    "        self.priorityKeywords=_prioritykeywords  \n",
    "        \n",
    "        if _save_to_folder==None or path.isdir(_save_to_folder)==False:\n",
    "            self.save_to_folder=os.getcwd()\n",
    "        else:\n",
    "            self.save_to_folder=_save_to_folder\n",
    "        \n",
    "        self.existingurlfile=path.join(self.save_to_folder,re.sub(\"\\s+?\", \"_\", self.college.strip()+'.csv'))\n",
    "            \n",
    "        self.respectRobottext=_respectrobottxt #currently not doing anything\n",
    "        \n",
    "        #self.allUrls=set()\n",
    "        #self.allRankedUrls = [] \n",
    "        self.allurls={}\n",
    "        self.rejectedUrls=set() \n",
    "        self.visitedUrls=set()\n",
    "        self.scheme=urlparse(self.rootUrl).scheme\n",
    "        self.base_domain=urlparse(self.rootUrl).netloc\n",
    "        self.files=[]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"************Crawl {}({})************\".format(self.college, self.rootUrl)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    def __getDomainFromUrl(self, url):\n",
    "        #url = \"http://www.mit.edu/\"\n",
    "        firstIndex = url.find('//')\n",
    "        length = len(url)-1 if url.endswith('/') else len(url)      \n",
    "        return url[firstIndex+2:length]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __addAllUrlsInOnePage(self, response):\n",
    "        soup=BeautifulSoup(response.text, 'html.parser') \n",
    "        for link in soup.find_all('a'): \n",
    "            if link.has_attr('href'):\n",
    "                url=link['href']       \n",
    "                url=re.sub(r'[\\/_+!@#$?\\\\\\s]+$', '', url)\n",
    "                parsed_uri = urlparse(url) \n",
    "                absolute_url=''\n",
    "           \n",
    "                if (parsed_uri.netloc=='') and (parsed_uri.scheme=='') and re.match(r'^\\/.*\\w$', parsed_uri.path) :\n",
    "                    absolute_url=urljoin(self.scheme+'://'+self.base_domain,parsed_uri.path)\n",
    "                elif parsed_uri.netloc==self.base_domain and re.match('^http', parsed_uri.scheme):\n",
    "                    absolute_url=url\n",
    "                elif parsed_uri.netloc==self.base_domain and parsed_uri.scheme==\"\" and re.match(r'^\\/.*\\w$', parsed_uri.path):\n",
    "                    absolute_url=self.scheme+'://'+parsed_uri.netloc+parsed_uri.path\n",
    "                else:\n",
    "                    continue \n",
    "                    \n",
    "                if absolute_url!='' and absolute_url not in urls:   \n",
    "                    urls[absolute_url]=0\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def getAllUrls(self):  \n",
    "        urls=self.Load_DiscoveredUrls() \n",
    "        if len(urls)==0:\n",
    "            urls={self.rootUrl:0}  \n",
    "            unvisited=[self.rootUrl]\n",
    "        else:\n",
    "            unvisited=[url for url, status_code in urls.items() if status_code==0]\n",
    "           # if not unvisited:\n",
    "           #    unvisited=[self.rootUrl]\n",
    "        \n",
    "        pages_visited =0\n",
    "        try:  \n",
    "            while unvisited:        \n",
    "                pages_visited+=1\n",
    "                if pages_visited>self.max_Pages:\n",
    "                    break \n",
    "                    \n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*100+10)/len(item))\n",
    "                url=unvisited.pop()  \n",
    "                response=requests.get(url, headers=headers)     \n",
    "                status_code=response.status_code\n",
    "                urls[url]=status_code\n",
    "                \n",
    "                # save all urls in the page to allurls dictionary with status code 0\n",
    "                contentType = response.headers.get('content-type')    \n",
    "                if status_code==200 and 'text/html' in contentType:\n",
    "                    self.__addAllUrlsInOnePage(response)\n",
    "                    \n",
    "                # write the page to csv\n",
    "                self.SaveToCsv_FromResponse(url, response)     \n",
    "             \n",
    "                unvisited=[name for name, code in urls.items() if code==0]          \n",
    "                time.sleep(self.Gap_Insecond) #wait for few seconds. \n",
    "          \n",
    "        except: \n",
    "            print('url \"{}\" went wrong'.format(url))  \n",
    "            urls[url]=999\n",
    "                #not to consider failed pages. status_code 400s may need manual handling of they are high priority pages\n",
    "        finally: \n",
    "            self.allurls=urls\n",
    "            #csv_columns = ['url', 'status_code']  \n",
    "            #try:\n",
    "                #with open(self.existingurlfile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    #writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                    #writer.writeheader()\n",
    "                    #for data in urls:\n",
    "                        #writer.writerow(data)\n",
    "            #except IOError:\n",
    "                #print(\"I/O error\")\n",
    "           # self.Save_Summaries()   \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "        filename, if not None, should not be full name. use import ntpath ntpath.basename(\"a\\b\\c\")\n",
    "    \"\"\"\n",
    "    def SaveToCsv_FromUrl(self,url): # tab delimiter only \n",
    "        url=url.strip() \n",
    "        content=self.Read_Oneurl(url) #in format of (a,a,a,a)   \n",
    "        self.SaveToCsv(url, content)\n",
    "        #return SaveToCsv_FromResponse()\n",
    "    \n",
    "    '''\n",
    "        save from resonse. called in the initial loop\n",
    "    '''\n",
    "    def SaveToCsv_FromResponse(self, url, response):\n",
    "        content= self.Get_Pagetext(response)\n",
    "        self.SaveToCsv(url, content) \n",
    "        \n",
    "    '''\n",
    "        save to csv file and append it to self.files\n",
    "    '''    \n",
    "    def SaveToCsv(self, url, content):\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "        fullname=path.join(self.save_to_folder, filename)\n",
    "        try:\n",
    "            with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "                for lll in content: \n",
    "                    lll.insert(0, url)\n",
    "                    writer.writerow(lll)  \n",
    "            self.files.append(fullname) \n",
    "        except IOError:\n",
    "            print('failed to save file.')\n",
    "           \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def read_OneUrl(self, url):\n",
    "        response=requests.get(url)  \n",
    "        if response.status_code==200:\n",
    "            if \"text/html\" in response.headers.get('content-type'):\n",
    "                return self.get_pagetext(response)\n",
    "            else: #for example .pdf file, img file\n",
    "                pass #save the file directly?\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "    \n",
    "    \n",
    "    def get_pagetext(self,body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser') #another type is content which is byte (like image)\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "    \n",
    "    def tag_visible(self,element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    def save_OnePage(self, url, format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "       # if folder==None:\n",
    "        folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder doesnot exist')\n",
    "            return\n",
    "        #if filename==None:\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='', encoding=\"utf-8\") as newFile:\n",
    "            writer = csv.writer(newFile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            if format != 'csv':\n",
    "               # writer = \n",
    "                pass\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
    "        #return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def save_Summaries(self):\n",
    "        print(self)\n",
    "        print(\"Total number of URLs visisted: \", len(self.visitedUrls))\n",
    "        print(\"Total number of URLs rejected: \", len(self.rejectedUrls))\n",
    "\n",
    "    \"\"\"\n",
    "        Ranks the urls based on the priority words\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    def setMaxpage(self, maxPages):\n",
    "        CollegeCrawl.max_Pages = maxPages\n",
    "        \n",
    "    def setGap(self, gap):\n",
    "        CollegeCrawl.gap_Insecond = gap\n",
    "    \n",
    "    \n",
    "    def crawl(self):\n",
    "        self.getAllUrls()\n",
    "        self.save_Summaries()\n",
    "        \n",
    "     \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def Load_DiscoveredUrls(self, delimiter=',', hasHeader=False, header_names=['url', 'status_code']):   \n",
    "        if self.existingurlfile==None:\n",
    "            return {}\n",
    "        else:\n",
    "            if path.exists(self.existingurlfile) and re.sub('\\s+', '_', self.college) in ntpath.basename(self.existingurlfile):\n",
    "                df_urls=pd.read_csv(self.existingurlfile,  delimiter=delimiter).iloc[:, 0:2]   \n",
    "                header_row=list(df_urls.columns) \n",
    "            \n",
    "                if re.match('^http', header_row[0]):\n",
    "                    df_urls.columns=header_names\n",
    "                    df_urls=pd.concat([df_urls, pd.DataFrame([header_row], columns=header_names)])                \n",
    "                else: #event it is not, still assign the header_name\n",
    "                    df_urls.columns=header_names \n",
    "                \n",
    "                return dict(zip(df_urls['url'], df_urls['status_code'])) #format: url:status_code. i.e., url is the key\n",
    "                #another options is: df_urls.to_dict('list') #format of {url:[url1, url2...], status_code:[0, 200...]}\n",
    "            else:\n",
    "                return {}        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colleges = [[\"UCLA\", \"http://www.ucla.edu\", []],\n",
    "           [\"Stanford\", \"https://www.stanford.edu/\", []],\n",
    "           [\"Yale\", \"https://www.yale.edu/\", []],\n",
    "           [\"UW\", \"http://www.washington.edu/\", []]]\n",
    "for college in colleges:\n",
    "    cr = CollegeCrawl(college[0], college[1], college[2])\n",
    "    cr.crawl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url)\n",
    "    #response = requests.get(url,None, headers=headers)\n",
    "    print(\"response status code for URL {} is {}\".format(url,response.status_code))\n",
    "    print(\"request headers for URL {} is {}\".format(url, response.request.headers))\n",
    "    \n",
    "    \n",
    "    #help(response)\n",
    "    #help(requests.get)\n",
    "    \n",
    "    print(response.text)\n",
    " \n",
    "\n",
    "\n",
    "getResponse(\"https://www.mit.edu\")\n",
    "#getResponseCode(\"http://www.stanford.edu\")\n",
    "#getResponseCode(\"http://www.harvard.edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ih[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
