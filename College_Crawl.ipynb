{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urlparse\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl:\n",
    "    gap_Insecond=5\n",
    "    max_Pages=5\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: www.university.edu\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then everth page \n",
    "        respectrobottxt: True\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, _prioritykeywords, _respectrobottxt=True):\n",
    "        self.college=_collegename\n",
    "        self.rootUrl=_rooturl\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt #currently not doing anything\n",
    "        self.allUrls=set()\n",
    "        self.allRankedUrls = []\n",
    "        self.visitedUrls=set() #all urls visited no matter what is the response status\n",
    "        self.rejectedUrls=set()\n",
    "        self.base_domain = self.__getDomainFromUrl(self.rootUrl)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"************Crawl {}({})************\".format(self.college, self.rootUrl)\n",
    "    \n",
    "        \n",
    "    \n",
    "    def __getDomainFromUrl(self, url):\n",
    "        #url = \"http://www.mit.edu\"\n",
    "        firstIndex = url.find('//')\n",
    "        length = len(url)-1 if url.endswith('/') else len(url)      \n",
    "        return url[firstIndex+2:length]\n",
    "    \n",
    "    def __addAllUrlsInOnePage(self, response):\n",
    "        soup=BeautifulSoup(response.text, 'html.parser')\n",
    "       \n",
    "        for link in soup.find_all('a'):\n",
    "            try:\n",
    "                url=link['href']                \n",
    "                parsed_uri = urlparse(url )\n",
    "                if parsed_uri.netloc=='':\n",
    "                    absolute_url = self.rootUrl+url    \n",
    "                elif parsed_uri.netloc==self.base_domain:\n",
    "                    absolute_url=url\n",
    "                else:\n",
    "                    continue\n",
    "                #netname = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri) \n",
    "                clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url)\n",
    "                #set_trace()\n",
    "                slashIndex = clean.index(\"//\") \n",
    "                clean=''.join(clean[0:slashIndex+1])+(''.join(clean[slashIndex+1:])).replace('//', '/')\n",
    "                clean=re.sub(r'[/\\s]$','', clean)\n",
    "                self.allUrls.add(clean)\n",
    "                    #unique_urls.add(re.sub(r'[_+!@#$?\\\\/^\\s]+$', '', absolute_url))                  \n",
    "            except:\n",
    "                continue                 \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def getAllUrls(self):          \n",
    "        self.allUrls.add(self.rootUrl)\n",
    "        while len(self.allUrls)>len(self.visitedUrls) and len(self.visitedUrls)<=CollegeCrawl.max_Pages:\n",
    "            #get one unvisited url\n",
    "            unvisited_url=(self.allUrls-self.visitedUrls).pop()\n",
    "               \n",
    "            # visit the unvisited url, parse the content and add all the urls to allUrls set\n",
    "            time.sleep(CollegeCrawl.gap_Insecond)\n",
    "            try:\n",
    "                # unvisited url becomes visited\n",
    "                self.visitedUrls.add(unvisited_url)\n",
    "                response=requests.get(unvisited_url,None, headers=CollegeCrawl.headers)\n",
    "                # check whether successful\n",
    "                if response.status_code !=200: \n",
    "                    self.rejectedUrls.add(unvisited_url)  \n",
    "                else:\n",
    "                    contentType = response.headers.get('content-type')\n",
    "                    #set_trace()\n",
    "                    if 'text/html' in contentType:\n",
    "                        self.__addAllUrlsInOnePage(response)\n",
    "            #except requests.exceptions.RequestException as e:\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def read_OneUrl(self, url):\n",
    "        response=requests.get(url)  \n",
    "        if response.status_code==200:\n",
    "            if \"text/html\" in response.headers.get('content-type'):\n",
    "                return self.get_pagetext(response)\n",
    "            else: #for example .pdf file, img file\n",
    "                pass #save the file directly?\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "    \n",
    "    \n",
    "    def get_pagetext(self,body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser') #another type is content which is byte (like image)\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "    \n",
    "    def tag_visible(self,element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    def save_OnePage(self, url, format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "       # if folder==None:\n",
    "        folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder doesnot exist')\n",
    "            return\n",
    "        #if filename==None:\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='', encoding=\"utf-8\") as newFile:\n",
    "            writer = csv.writer(newFile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            if format != 'csv':\n",
    "               # writer = \n",
    "                pass\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
    "        #return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def save_Summaries(self):\n",
    "        print(self)\n",
    "        print(\"Total number of URLs visisted: \", len(self.visitedUrls))\n",
    "        print(\"Total number of URLs rejected: \", len(self.rejectedUrls))\n",
    "        print(\"URLs visited (ranked with priority words):\\n\",self.allRankedUrls)\n",
    "    \n",
    "    \"\"\"\n",
    "        Ranks the urls based on the priority words\n",
    "    \"\"\"\n",
    "    def getRankedVisitedUrls(self):\n",
    "        self.allRankedUrls = sorted(self.visitedUrls,key=lambda url: [w in url for w in self.priorityKeywords].count(True),reverse=True)\n",
    "        return self.allRankedUrls\n",
    "    \n",
    "    def setMaxpage(self, maxPages):\n",
    "        CollegeCrawl.max_Pages = maxPages\n",
    "        \n",
    "    def setGap(self, gap):\n",
    "        CollegeCrawl.gap_Insecond = gap\n",
    "    \n",
    "    \n",
    "    def crawl(self):\n",
    "        self.getAllUrls()\n",
    "        rankedUrlList = self.getRankedVisitedUrls()\n",
    "        for url in rankedUrlList:\n",
    "            self.save_OnePage(url)\n",
    "            \n",
    "        self.save_Summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges = [[\"UCLA\", \"http://www.ucla.edu\", [\"apply\",\"admission\",\"research\"]],\n",
    "           [\"Stanford\", \"https://www.stanford.edu/\", [\"apply\",\"admission\",\"research\"]],\n",
    "           [\"Yale\", \"https://www.yale.edu/\", [\"apply\",\"admission\",\"research\"]],\n",
    "           [\"UW\", \"http://www.washington.edu/\", [\"apply\",\"admission\",\"research\"]]]\n",
    "for college in colleges:\n",
    "    cr = CollegeCrawl(college[0], college[1], college[2])\n",
    "    cr.crawl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
