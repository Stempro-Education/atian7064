{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urlparse\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl:\n",
    "    gap_Insecond=5\n",
    "    max_Pages=5\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,_collegename, _rooturl, \n",
    "            _prioritykeywords=['apply', 'admission', 'application', 'deadline'], \n",
    "            _url_file=None,                  \n",
    "            _save_to_folder=None,\n",
    "            _existingurlfile=None, #csv files that were visited in the past\n",
    "            _respectrobottxt=True, \n",
    "            _headers={'User-Agent':'Mozilla/5.0'} ):\n",
    "        self.college=_collegename\n",
    "         \n",
    "        if urlparse(_rooturl).scheme==\"\":\n",
    "            print('URL needs to have scheme. Please try again.')\n",
    "            raise Exception('URL needs to have scheme')            \n",
    "            return\n",
    "        \n",
    "        self.save_to_folder=None\n",
    "        self.college=_collegename\n",
    "        self.rootUrl=_rooturl\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt #currently not doing anything\n",
    "        self.allUrls=set()\n",
    "        self.allRankedUrls = [] \n",
    "        self.existingurlfile=None\n",
    "        self.rejectedUrls=set()\n",
    "        self.base_domain = self.__getDomainFromUrl(self.rootUrl)\n",
    "        self.visitedUrls=set()\n",
    "        self.scheme=urlparse(self.rootUrl).scheme\n",
    "        \n",
    "        \n",
    "        if _save_to_folder==None or path.isdir(_save_to_folder)==False:\n",
    "            self.save_to_folder=os.getcwd()\n",
    "        else:\n",
    "            self.save_to_folder=_save_to_folder\n",
    "            \n",
    "        #to make it less \n",
    "        if _existingurlfile==None or path.exists(_existingurlfile)==False:            \n",
    "            self.existingurlfile=path.join(self.save_to_folder,re.sub(\"\\s+\", \"_\", self.college.strip()+'.csv'))\n",
    "        else:\n",
    "            self.existingurlfile=_existingurlfile\n",
    "        self.allurls={}\n",
    "        self.headers=_headers\n",
    "        self.files=[]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"************Crawl {}({})************\".format(self.college, self.rootUrl)\n",
    "    \n",
    "        \n",
    "    \n",
    "    def __getDomainFromUrl(self, url):\n",
    "        #url = \"http://www.mit.edu/\"\n",
    "        firstIndex = url.find('//')\n",
    "        length = len(url)-1 if url.endswith('/') else len(url)      \n",
    "        return url[firstIndex+2:length]\n",
    "    \n",
    "    def __addAllUrlsInOnePage(self, response):\n",
    "        soup=BeautifulSoup(response.text, 'html.parser')\n",
    "       \n",
    "        for link in soup.find_all('a'):\n",
    "            try:\n",
    "                url=link['href']                \n",
    "                parsed_uri = urlparse(url )\n",
    "                if parsed_uri.netloc=='':\n",
    "                    absolute_url = self.rootUrl+url    \n",
    "                elif parsed_uri.netloc==self.base_domain:\n",
    "                    absolute_url=url\n",
    "                else:\n",
    "                    continue\n",
    "                #netname = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri) \n",
    "                clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url)\n",
    "                #set_trace()\n",
    "                slashIndex = clean.index(\"//\") \n",
    "                clean=''.join(clean[0:slashIndex+1])+(''.join(clean[slashIndex+1:])).replace('//', '/')\n",
    "                clean=re.sub(r'[/\\s]?$','', clean)\n",
    "                self.allUrls.add(clean)\n",
    "                    #unique_urls.add(re.sub(r'[_+!@#$?\\\\/^\\s]+$', '', absolute_url))                  \n",
    "            except:\n",
    "                print(\"There was an error\")\n",
    "                continue                 \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def getAllUrls(self):  \n",
    "        urls=self.Load_DiscoveredUrls() \n",
    "        self.allUrls.add(self.rootUrl)\n",
    "        while len(self.allUrls)>len(self.visitedUrls) and len(self.visitedUrls)<=CollegeCrawl.max_Pages:\n",
    "            #get one unvisited url\n",
    "            unvisited_url=(self.allUrls-self.visitedUrls).pop()\n",
    "               \n",
    "            # visit the unvisited url, parse the content and add all the urls to allUrls set\n",
    "            time.sleep(CollegeCrawl.gap_Insecond)\n",
    "            try:\n",
    "                # unvisited url becomes visited\n",
    "                self.visitedUrls.add(unvisited_url)\n",
    "                response=requests.get(unvisited_url,None, headers=CollegeCrawl.headers)\n",
    "                # check whether successful\n",
    "                if response.status_code !=200: \n",
    "                    self.rejectedUrls.add(unvisited_url)  \n",
    "                else:\n",
    "                    contentType = response.headers.get('content-type')\n",
    "                    #set_trace()\n",
    "                    if 'text/html' in contentType:\n",
    "                        self.__addAllUrlsInOnePage(response)\n",
    "                \n",
    "                unvisited=[name for name, response.status.code in urls.items() if response.status.code==0]\n",
    "                self.allRankedUrls = sorted(self.visitedUrls,key=lambda url: [w in url for w in self.priorityKeywords].count(True),reverse=True)\n",
    "\n",
    "            #except requests.exceptions.RequestException as e:\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def read_OneUrl(self, url):\n",
    "        response=requests.get(url)  \n",
    "        if response.status_code==200:\n",
    "            if \"text/html\" in response.headers.get('content-type'):\n",
    "                return self.get_pagetext(response)\n",
    "            else: #for example .pdf file, img file\n",
    "                pass #save the file directly?\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "    \n",
    "    \n",
    "    def get_pagetext(self,body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser') #another type is content which is byte (like image)\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "    \n",
    "    def tag_visible(self,element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    def save_OnePage(self, url, format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "       # if folder==None:\n",
    "        folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder doesnot exist')\n",
    "            return\n",
    "        #if filename==None:\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='', encoding=\"utf-8\") as newFile:\n",
    "            writer = csv.writer(newFile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            if format != 'csv':\n",
    "               # writer = \n",
    "                pass\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
    "        #return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def save_Summaries(self):\n",
    "        print(self)\n",
    "        print(\"Total number of URLs visisted: \", len(self.visitedUrls))\n",
    "        print(\"Total number of URLs rejected: \", len(self.rejectedUrls))\n",
    "        print(\"URLs visited (ranked with priority words):\\n\",self.allRankedUrls)\n",
    "    \n",
    "    \"\"\"\n",
    "        Ranks the urls based on the priority words\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    def setMaxpage(self, maxPages):\n",
    "        CollegeCrawl.max_Pages = maxPages\n",
    "        \n",
    "    def setGap(self, gap):\n",
    "        CollegeCrawl.gap_Insecond = gap\n",
    "    \n",
    "    \n",
    "    def crawl(self):\n",
    "        self.getAllUrls()\n",
    "        for url in self.allRankedUrls:\n",
    "            self.save_OnePage(url)\n",
    "            \n",
    "        self.save_Summaries()\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def Load_DiscoveredUrls(self, delimiter=',', hasHeader=False, header_names=['url', 'status_code']):   \n",
    "        if self.existingurlfile==None:\n",
    "            return {}\n",
    "        else:\n",
    "            if path.exists(self.existingurlfile) and re.sub('\\s+', '_', self.college) in ntpath.basename(self.existingurlfile):\n",
    "                df_urls=pd.read_csv(self.existingurlfile,  delimiter=delimiter).iloc[:, 0:2]   \n",
    "                header_row=list(df_urls.columns) \n",
    "            \n",
    "                if re.match('^http', header_row[0]):\n",
    "                    df_urls.columns=header_names\n",
    "                    df_urls=pd.concat([df_urls, pd.DataFrame([header_row], columns=header_names)])                \n",
    "                else: #event it is not, still assign the header_name\n",
    "                    df_urls.columns=header_names \n",
    "                \n",
    "                return dict(zip(df_urls['url'], df_urls['status_code'])) #format: url:status_code. i.e., url is the key\n",
    "                #another options is: df_urls.to_dict('list') #format of {url:[url1, url2...], status_code:[0, 200...]}\n",
    "            else:\n",
    "                return {}        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an error\n",
      "************Crawl UCLA(http://www.ucla.edu)************\n",
      "Total number of URLs visisted:  6\n",
      "Total number of URLs rejected:  0\n",
      "URLs visited (ranked with priority words):\n",
      " ['http://www.ucla.edu/research/research-across-campus', 'http://www.ucla.edu/campus-life', 'http://www.ucla.edu/optimists/fisher', 'http://www.ucla.edu/about/iconic-and-influential-alumni', 'http://www.ucla.edu/academics', 'http://www.ucla.edu']\n",
      "************Crawl Stanford(https://www.stanford.edu/)************\n",
      "Total number of URLs visisted:  6\n",
      "Total number of URLs rejected:  0\n",
      "URLs visited (ranked with priority words):\n",
      " ['https://www.stanford.edu/#b', 'https://www.stanford.edu/#y', 'https://www.stanford.edu/atoz', 'https://www.stanford.edu/#h', 'https://www.stanford.edu/#v', 'https://www.stanford.edu/']\n",
      "************Crawl Yale(https://www.yale.edu/)************\n",
      "Total number of URLs visisted:  6\n",
      "Total number of URLs rejected:  0\n",
      "URLs visited (ranked with priority words):\n",
      " ['https://www.yale.edu/life-yale/new-haven', 'https://www.yale.edu/academics/undergraduate-study', 'https://www.yale.edu/calendars', 'https://www.yale.edu/research-collections/museums-galleries', 'https://www.yale.edu/life-yale', 'https://www.yale.edu/']\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "There was an error\n",
      "************Crawl UW(http://www.washington.edu/)************\n",
      "Total number of URLs visisted:  6\n",
      "Total number of URLs rejected:  1\n",
      "URLs visited (ranked with priority words):\n",
      " ['http://www.washington.edu', 'http://www.washington.edu/safety', 'http://www.washington.edu/FCCH24.html', 'http://www.washington.edu/safety/emergency-communications', 'https://www.washington.edu/admin/rules/policies/FCG/FCCH25.html#2571', 'http://www.washington.edu/']\n"
     ]
    }
   ],
   "source": [
    "colleges = [[\"UCLA\", \"http://www.ucla.edu\", []],\n",
    "           [\"Stanford\", \"https://www.stanford.edu/\", []],\n",
    "           [\"Yale\", \"https://www.yale.edu/\", []],\n",
    "           [\"UW\", \"http://www.washington.edu/\", []]]\n",
    "for college in colleges:\n",
    "    cr = CollegeCrawl(college[0], college[1], college[2])\n",
    "    cr.crawl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url)\n",
    "    #response = requests.get(url,None, headers=headers)\n",
    "    print(\"response status code for URL {} is {}\".format(url,response.status_code))\n",
    "    print(\"request headers for URL {} is {}\".format(url, response.request.headers))\n",
    "    \n",
    "    \n",
    "    #help(response)\n",
    "    #help(requests.get)\n",
    "    \n",
    "    print(response.text)\n",
    " \n",
    "\n",
    "\n",
    "getResponse(\"https://www.mit.edu\")\n",
    "#getResponseCode(\"http://www.stanford.edu\")\n",
    "#getResponseCode(\"http://www.harvard.edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ih[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
