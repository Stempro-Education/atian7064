{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
<<<<<<< HEAD
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n"
=======
    "import csv\n",
    "from IPython.core.debugger import set_trace"
>>>>>>> parent of 9d9eea6... College crawl update
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl:\n",
    "    gap_Insecond=5\n",
    "    max_Pages=5\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,_collegename, _rooturl, \n",
    "            _prioritykeywords=['apply', 'admission', 'application', 'deadline'], \n",
    "           # _url_file=None,            \n",
    "            _save_to_folder=None,\n",
    "            _respectrobottxt=True):\n",
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl(object):\n",
    "    gap_Insecond=0\n",
    "    max_Pages=10\n",
>>>>>>> parent of 9d9eea6... College crawl update
    "    \n",
    "        self.college=_collegename\n",
    "         \n",
    "        if urlparse(_rooturl).scheme==\"\":\n",
    "            print('URL needs to have scheme. Please try again.')\n",
    "            raise Exception('URL needs to have scheme')            \n",
    "            return\n",
    "        self.rootUrl=_rooturl\n",
<<<<<<< HEAD
    "        \n",
    "        self.priorityKeywords=_prioritykeywords  \n",
    "        \n",
    "        if _save_to_folder==None or path.isdir(_save_to_folder)==False:\n",
    "            self.save_to_folder=os.getcwd()\n",
    "        else:\n",
    "            self.save_to_folder=_save_to_folder\n",
    "        \n",
    "        self.existingurlfile=path.join(self.save_to_folder,re.sub(\"\\s+?\", \"_\", self.college.strip()+'.csv'))\n",
    "            \n",
    "        self.respectRobottext=_respectrobottxt #currently not doing anything\n",
    "        \n",
    "        #self.allUrls=set()\n",
    "        #self.allRankedUrls = [] \n",
    "        self.allurls={}\n",
    "        self.rejectedUrls=set() \n",
    "        self.visitedUrls=set()\n",
    "        self.scheme=urlparse(self.rootUrl).scheme\n",
    "        self.base_domain=urlparse(self.rootUrl).netloc\n",
    "        self.files=[]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"************Crawl {}({})************\".format(self.college, self.rootUrl)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    def __getDomainFromUrl(self, url):\n",
    "        #url = \"http://www.mit.edu/\"\n",
    "        firstIndex = url.find('//')\n",
    "        length = len(url)-1 if url.endswith('/') else len(url)      \n",
    "        return url[firstIndex+2:length]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __addAllUrlsInOnePage(self, response):\n",
    "        soup=BeautifulSoup(response.text, 'html.parser') \n",
    "        for link in soup.find_all('a'): \n",
    "            if link.has_attr('href'):\n",
    "                url=link['href']       \n",
    "                url=re.sub(r'[\\/_+!@#$?\\\\\\s]+$', '', url)\n",
    "                parsed_uri = urlparse(url) \n",
    "                absolute_url=''\n",
    "           \n",
    "                if (parsed_uri.netloc=='') and (parsed_uri.scheme=='') and re.match(r'^\\/.*\\w$', parsed_uri.path) :\n",
    "                    absolute_url=urljoin(self.scheme+'://'+self.base_domain,parsed_uri.path)\n",
    "                elif parsed_uri.netloc==self.base_domain and re.match('^http', parsed_uri.scheme):\n",
    "                    absolute_url=url\n",
    "                elif parsed_uri.netloc==self.base_domain and parsed_uri.scheme==\"\" and re.match(r'^\\/.*\\w$', parsed_uri.path):\n",
    "                    absolute_url=self.scheme+'://'+parsed_uri.netloc+parsed_uri.path\n",
    "                else:\n",
    "                    continue \n",
    "                    \n",
    "                if absolute_url!='' and absolute_url not in urls:   \n",
    "                    urls[absolute_url]=0\n",
    "        \n",
=======
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "        self.allUrls=[]\n",
    "        self.visitedUrls=[]\n",
    "        self.rejectedUrls=[]\n",
    "        self.n_allUrls=0\n",
    "        self.n_sitedUrls=0\n",
    "        self.n_rejectedUrls=0\n",
    "    \n",
    "        \n",
>>>>>>> parent of 9d9eea6... College crawl update
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
<<<<<<< HEAD
    "    def getAllUrls(self):  \n",
    "        urls=self.Load_DiscoveredUrls() \n",
    "        if len(urls)==0:\n",
    "            urls={self.rootUrl:0}  \n",
    "            unvisited=[self.rootUrl]\n",
    "        else:\n",
    "            unvisited=[url for url, status_code in urls.items() if status_code==0]\n",
    "           # if not unvisited:\n",
    "           #    unvisited=[self.rootUrl]\n",
    "        \n",
    "        pages_visited =0\n",
    "        try:  \n",
    "            while unvisited:        \n",
    "                pages_visited+=1\n",
    "                if pages_visited>self.max_Pages:\n",
    "                    break \n",
    "                    \n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*100+10)/len(item))\n",
    "                url=unvisited.pop()  \n",
    "                response=requests.get(url, headers=headers)     \n",
    "                status_code=response.status_code\n",
    "                urls[url]=status_code\n",
    "                \n",
    "                # save all urls in the page to allurls dictionary with status code 0\n",
    "                contentType = response.headers.get('content-type')    \n",
    "                if status_code==200 and 'text/html' in contentType:\n",
    "                    self.__addAllUrlsInOnePage(response)\n",
    "                    \n",
    "                # write the page to csv\n",
    "                self.SaveToCsv_FromResponse(url, response)     \n",
    "             \n",
    "                unvisited=[name for name, code in urls.items() if code==0]          \n",
    "                time.sleep(self.Gap_Insecond) #wait for few seconds. \n",
    "          \n",
    "        except: \n",
    "            print('url \"{}\" went wrong'.format(url))  \n",
    "            urls[url]=999\n",
    "                #not to consider failed pages. status_code 400s may need manual handling of they are high priority pages\n",
    "        finally: \n",
    "            self.allurls=urls\n",
    "            #csv_columns = ['url', 'status_code']  \n",
    "            #try:\n",
    "                #with open(self.existingurlfile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    #writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                    #writer.writeheader()\n",
    "                    #for data in urls:\n",
    "                        #writer.writerow(data)\n",
    "            #except IOError:\n",
    "                #print(\"I/O error\")\n",
    "           # self.Save_Summaries()   \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "        filename, if not None, should not be full name. use import ntpath ntpath.basename(\"a\\b\\c\")\n",
    "    \"\"\"\n",
    "    def SaveToCsv_FromUrl(self,url): # tab delimiter only \n",
    "        url=url.strip() \n",
    "        content=self.Read_Oneurl(url) #in format of (a,a,a,a)   \n",
    "        self.SaveToCsv(url, content)\n",
    "        #return SaveToCsv_FromResponse()\n",
    "    \n",
    "    '''\n",
    "        save from resonse. called in the initial loop\n",
    "    '''\n",
    "    def SaveToCsv_FromResponse(self, url, response):\n",
    "        content= self.Get_Pagetext(response)\n",
    "        self.SaveToCsv(url, content) \n",
    "        \n",
    "    '''\n",
    "        save to csv file and append it to self.files\n",
    "    '''    \n",
    "    def SaveToCsv(self, url, content):\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "        fullname=path.join(self.save_to_folder, filename)\n",
    "        try:\n",
    "            with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "                for lll in content: \n",
    "                    lll.insert(0, url)\n",
    "                    writer.writerow(lll)  \n",
    "            self.files.append(fullname) \n",
    "        except IOError:\n",
    "            print('failed to save file.')\n",
    "           \n",
    "        \n",
    "    \n",
=======
    "    def all_pages(self, base_url, links_only=True):\n",
    "        response=requests.get(base_url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "        unique_urls={base_url}\n",
    "        visited_urls=set()\n",
    "        unables=set()\n",
    "        while len(unique_urls)>len(visited_urls):\n",
    "            soup=BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                try:\n",
    "                    url=link['href']                \n",
    "                    parsed_uri = urlparse(url )\n",
    "                    if parsed_uri.netloc=='':\n",
    "                        absolute_url=base_url+url    \n",
    "                    elif parsed_uri.netloc==base_domain:\n",
    "                        absolute_url=url\n",
    "                    else:\n",
    "                        continue\n",
    "                    clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url)\n",
    "                    clean=''.join(clean[0:11])+(''.join(clean[11:])).replace('//', '/')\n",
    "                    clean=re.sub(r'[/\\s]$','', clean)\n",
    "                    unique_urls.add(clean)           \n",
    "                except:\n",
    "                    continue            \n",
    "            unvisited_url=(unique_urls-visited_urls-unables).pop()\n",
    "            visited_urls.add(unvisited_url)\n",
    "            response=requests.get(unvisited_url)\n",
    "            if links_only!=True and response.status_code== 200: #use a different routine to get text data from resonse. \n",
    "                handleOnePage(unvisited_url)\n",
    "            else:\n",
    "                unables.add(unvisited_url)  \n",
    "        return unique_urls\n",
>>>>>>> parent of 9d9eea6... College crawl update
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def read_OneUrl(self, url):\n",
    "        response=requests.get(url)  \n",
    "        if response.status_code==200: \n",
    "            return self.get_pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "            \n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "\n",
    "    def get_pagetext(self, body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "        \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    \n",
    "    def save_OnePage(self,url,folder=None, filename=None, format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "        if folder==None:\n",
    "            folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder doesnot exist')\n",
    "            return\n",
    "        if filename==None:\n",
    "            filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
<<<<<<< HEAD
    "        #return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def save_Summaries(self):\n",
    "        print(self)\n",
    "        print(\"Total number of URLs visisted: \", len(self.visitedUrls))\n",
    "        print(\"Total number of URLs rejected: \", len(self.rejectedUrls))\n",
    "\n",
    "    \"\"\"\n",
    "        Ranks the urls based on the priority words\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    def setMaxpage(self, maxPages):\n",
    "        CollegeCrawl.max_Pages = maxPages\n",
    "        \n",
    "    def setGap(self, gap):\n",
    "        CollegeCrawl.gap_Insecond = gap\n",
    "    \n",
    "    \n",
    "    def crawl(self):\n",
    "        self.getAllUrls()\n",
    "        self.save_Summaries()\n",
    "        \n",
    "     \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def Load_DiscoveredUrls(self, delimiter=',', hasHeader=False, header_names=['url', 'status_code']):   \n",
    "        if self.existingurlfile==None:\n",
    "            return {}\n",
    "        else:\n",
    "            if path.exists(self.existingurlfile) and re.sub('\\s+', '_', self.college) in ntpath.basename(self.existingurlfile):\n",
    "                df_urls=pd.read_csv(self.existingurlfile,  delimiter=delimiter).iloc[:, 0:2]   \n",
    "                header_row=list(df_urls.columns) \n",
    "            \n",
    "                if re.match('^http', header_row[0]):\n",
    "                    df_urls.columns=header_names\n",
    "                    df_urls=pd.concat([df_urls, pd.DataFrame([header_row], columns=header_names)])                \n",
    "                else: #event it is not, still assign the header_name\n",
    "                    df_urls.columns=header_names \n",
    "                \n",
    "                return dict(zip(df_urls['url'], df_urls['status_code'])) #format: url:status_code. i.e., url is the key\n",
    "                #another options is: df_urls.to_dict('list') #format of {url:[url1, url2...], status_code:[0, 200...]}\n",
    "            else:\n",
    "                return {}        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colleges = [[\"UCLA\", \"http://www.ucla.edu\", []],\n",
    "           [\"Stanford\", \"https://www.stanford.edu/\", []],\n",
    "           [\"Yale\", \"https://www.yale.edu/\", []],\n",
    "           [\"UW\", \"http://www.washington.edu/\", []]]\n",
    "for college in colleges:\n",
    "    cr = CollegeCrawl(college[0], college[1], college[2])\n",
    "    cr.crawl()\n"
=======
    "        return fullname           \n",
    "    \n",
    "\n",
    "     "
>>>>>>> parent of 9d9eea6... College crawl update
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "names=['UCLA','Yale','University of Washington','Stanford']\n",
    "urls=['http://www.ucla.edu/','http://yale.edu/','http://washington.edu/','http://stanford.edu/']\n",
    "k=['apply,admission']\n",
    "for q in range(len(names)):\n",
    "    c=CollegeCrawl(names[q],urls[q],k)\n",
    "    a=c.all_pages(urls[q])\n",
    "    for x in a:\n",
    "        c.save_OnePage(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers sent for URL http://www.mit.edu is {'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
      "response status code for URL http://www.mit.edu is 200\n",
      "Help on function get in module requests.api:\n",
      "\n",
      "get(url, params=None, **kwargs)\n",
      "    Sends a GET request.\n",
      "    \n",
      "    :param url: URL for the new :class:`Request` object.\n",
      "    :param params: (optional) Dictionary, list of tuples or bytes to send\n",
      "        in the query string for the :class:`Request`.\n",
      "    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n",
      "    :return: :class:`Response <Response>` object\n",
      "    :rtype: requests.Response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#response=requests.get(\"https://stanford.edu\", None, headers={'User-Agent': \"*\"})\n",
    "#response=requests.get(\"https://stanford.edu\", headers={'User-Agent':'Mozilla/5.0'})\n",
    "url = \"http://www.mit.edu\"\n",
    "response = requests.get(url)\n",
    "print(\"headers sent for URL {} is {}\".format(url, response.request.headers))\n",
    "print(\"response status code for URL {} is {}\".format(url, response.status_code))\n",
    "help(requests.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
