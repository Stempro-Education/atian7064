{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChanges:\\n1. Changed save_Summaries, also prints visited urls\\n2. Automatically creates a new folder under current directory and saves generated files in that folder instead\\n3. Improved exception handing, now also prints exception encountered\\n4. Squashed some bugs \\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import ntpath\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\"\"\"\n",
    "Changes:\n",
    "1. Changed save_Summaries, also prints visited urls\n",
    "2. Automatically creates a new folder under current directory and saves generated files in that folder instead\n",
    "3. Improved exception handing, now also prints exception encountered\n",
    "4. Squashed some bugs \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl:\n",
    "    gap_Insecond=0\n",
    "    max_Pages=5\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,_collegename, _rooturl, \n",
    "            _prioritykeywords=['apply', 'admission', 'application', 'deadline'], \n",
    "           # _url_file=None,            \n",
    "            _save_to_folder=None,\n",
    "            _respectrobottxt=True):\n",
    "    \n",
    "        self.college=_collegename\n",
    "         \n",
    "        if urlparse(_rooturl).scheme==\"\":\n",
    "            print('URL needs to have scheme. Please try again.')\n",
    "            raise Exception('URL needs to have scheme')            \n",
    "            return\n",
    "        self.rootUrl=_rooturl\n",
    "        \n",
    "        self.priorityKeywords=_prioritykeywords  \n",
    "        \n",
    "        toSaveDir = os.path.join(os.getcwd(),\"collegeFiles\")\n",
    "        if not os.path.exists(toSaveDir):\n",
    "            os.makedirs(toSaveDir)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if _save_to_folder==None or path.isdir(_save_to_folder)==False:\n",
    "            self.save_to_folder=toSaveDir\n",
    "        else:\n",
    "            self.save_to_folder=toSaveDir\n",
    "            \n",
    "\n",
    "        \n",
    "        self.existingurlfile=path.join(self.save_to_folder,re.sub(\"\\s+?\", \"_\", self.college.strip()+'.csv'))\n",
    "            \n",
    "        self.respectRobottext=_respectrobottxt #currently not doing anything\n",
    "        \n",
    "        #self.allUrls=set()\n",
    "        #self.allRankedUrls = [] \n",
    "        self.allurls={}\n",
    "        #self.rejectedUrls=set() \n",
    "        #self.visitedUrls=set()\n",
    "        self.scheme=urlparse(self.rootUrl).scheme\n",
    "        self.base_domain=urlparse(self.rootUrl).netloc\n",
    "        self.files=[]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"************Crawl {}({})************\".format(self.college, self.rootUrl)\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    def __getDomainFromUrl(self, url):\n",
    "        #url = \"http://www.mit.edu/\"\n",
    "        firstIndex = url.find('//')\n",
    "        length = len(url)-1 if url.endswith('/') else len(url)      \n",
    "        return url[firstIndex+2:length]\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "     Read college-visiting csv file and put the content to dictionary{url:status_code}\n",
    "     return dictionary {url:status_code}\n",
    "    \"\"\"\n",
    "    def __load_DiscoveredUrls(self, delimiter=',', hasHeader=False, header_names=['url', 'status_code']):   \n",
    "        if self.existingurlfile==None:\n",
    "            return {}\n",
    "        else:\n",
    "            if path.exists(self.existingurlfile) and re.sub('\\s+?', '_', self.college) in ntpath.basename(self.existingurlfile):\n",
    "                df_urls=pd.read_csv(self.existingurlfile,  delimiter=delimiter).iloc[:, 0:2]   \n",
    "                header_row=list(df_urls.columns) \n",
    "            \n",
    "                if re.match('^http', header_row[0]):\n",
    "                    df_urls.columns=header_names\n",
    "                    df_urls=pd.concat([df_urls, pd.DataFrame([header_row], columns=header_names)])                \n",
    "                else: #even it is not, still assign the header_name\n",
    "                    df_urls.columns=header_names \n",
    "                \n",
    "                return dict(zip(df_urls['url'], df_urls['status_code'])) #format: url:status_code. i.e., url is the key\n",
    "                #another options is: df_urls.to_dict('list') #format of {url:[url1, url2...], status_code:[0, 200...]}\n",
    "            else:\n",
    "                return {}        \n",
    "    \n",
    "    def __addAllUrlsInOnePage(self, soup):\n",
    "        #soup=BeautifulSoup(response.text, 'html.parser') \n",
    "        for link in soup.find_all('a'): \n",
    "            if link.has_attr('href'):\n",
    "                url=link['href']       \n",
    "                url=re.sub(r'[\\/_+!@#$?\\\\\\s]+$', '', url)\n",
    "                parsed_uri = urlparse(url) \n",
    "                absolute_url=''\n",
    "           \n",
    "                if (parsed_uri.netloc=='') and (parsed_uri.scheme=='') and re.match(r'^\\/.*\\w$', parsed_uri.path) :\n",
    "                    absolute_url=urljoin(self.scheme+'://'+self.base_domain,parsed_uri.path)\n",
    "                elif parsed_uri.netloc==self.base_domain and re.match('^http', parsed_uri.scheme):\n",
    "                    absolute_url=url\n",
    "                elif parsed_uri.netloc==self.base_domain and parsed_uri.scheme==\"\" and re.match(r'^\\/.*\\w$', parsed_uri.path):\n",
    "                    absolute_url=self.scheme+'://'+parsed_uri.netloc+parsed_uri.path\n",
    "                else:\n",
    "                    continue \n",
    "                    \n",
    "                if absolute_url!='' and absolute_url not in self.allurls:   \n",
    "                    self.allurls[absolute_url]=0\n",
    "                    \n",
    "    def __get_Pagetext(self,soup):\n",
    "        #soup = BeautifulSoup(body.text, 'html.parser') #another type is content which is byte (like image)\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.__tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "    \n",
    "    def __tag_visible(self,element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "     \n",
    "    \n",
    "    \n",
    "    '''\n",
    "        save from resonse. called in the initial loop\n",
    "    '''\n",
    "    def __saveToCsv_FromResponse(self, url, soap):\n",
    "        content= self.__get_Pagetext(soap)\n",
    "        self.__saveToCsv(url, content) \n",
    "        \n",
    "    '''\n",
    "        save to csv file and append it to self.files\n",
    "    '''    \n",
    "    def __saveToCsv(self, url, content):\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace('?', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "        fullname=path.join(self.save_to_folder, filename)\n",
    "        try:\n",
    "            with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "                for lll in content: \n",
    "                    lll.insert(0, url)\n",
    "                    writer.writerow(lll)  \n",
    "            self.files.append(fullname) \n",
    "        except IOError as e:\n",
    "            print('failed to save file.')\n",
    "            print(\"Exception is: \", e)\n",
    "            \n",
    "    def __save_url_file(self):\n",
    "         #save file as well  \n",
    "            try:\n",
    "                with open(self.existingurlfile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "                    writer.writerow(['url','status_code'])\n",
    "                    for url, status_code in self.allurls.items():  \n",
    "                        writer.writerow([url, status_code])  \n",
    "            except IOError:\n",
    "                print('IO Error in saving summaries')\n",
    " \n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def getAllUrls(self):  \n",
    "        self.allurls=self.__load_DiscoveredUrls() \n",
    "        if len(self.allurls)==0:\n",
    "            self.allurls={self.rootUrl:0}  \n",
    "            unvisited=[self.rootUrl]\n",
    "        else:\n",
    "            unvisited=[url for url, status_code in self.allurls.items() if status_code==0]\n",
    "            # not necessary if priority words do not change\n",
    "            if not unvisited:\n",
    "                unvisited=[self.rootUrl]\n",
    "        \n",
    "        pages_visited =0\n",
    "        try:  \n",
    "            while unvisited:\n",
    "                if pages_visited==CollegeCrawl.max_Pages:\n",
    "                    break \n",
    "                pages_visited+=1         \n",
    "                    \n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*100+10)/len(item))\n",
    "                url=unvisited.pop()  \n",
    "                response=requests.get(url, headers=CollegeCrawl.headers)     \n",
    "                status_code=response.status_code\n",
    "                self.allurls[url]=status_code\n",
    "                \n",
    "                # save all urls in the page to allurls dictionary with status code 0\n",
    "                contentType = response.headers.get('content-type')    \n",
    "                if status_code==200 and 'text/html' in contentType:\n",
    "                    soup=BeautifulSoup(response.text, 'html.parser') \n",
    "                    \n",
    "                    self.__addAllUrlsInOnePage(soup)\n",
    "                    \n",
    "                    # write the page to csv\n",
    "                    self.__saveToCsv_FromResponse(url, soup)     \n",
    "             \n",
    "                unvisited=[name for name, code in self.allurls.items() if code==0]\n",
    "                time.sleep(CollegeCrawl.gap_Insecond) #wait for few seconds. \n",
    "          \n",
    "        except Exception as e: \n",
    "            print('url \"{}\" went wrong'.format(url))  \n",
    "            print(e)\n",
    "            self.allurls[url]=999\n",
    "                \n",
    "        \n",
    "   \n",
    "           \n",
    "    \"\"\"\n",
    "        display summaries \n",
    "    \"\"\"\n",
    "    \n",
    "    def save_Summaries(self):\n",
    "        if self.allurls:\n",
    "            df_urls=pd.DataFrame(list(self.allurls.items()), columns=['url', 'status_code'])\n",
    "            print(self)\n",
    "            \n",
    "            print(\"Summary of status code for URLs so far\")\n",
    "            print(df_urls.groupby('status_code').count().reset_index())\n",
    "            print('\\n')\n",
    "            print(\"Visited URLs so far, including previously visited URLs\")\n",
    "            visited=[name for name, code in self.allurls.items() if code!=0]\n",
    "            print(visited)\n",
    "            \n",
    "            \n",
    "            self.__save_url_file()\n",
    "                \n",
    "        if self.files:\n",
    "            print('\\nThe following {} file(s) are generated this run. '.format(len(self.files)))\n",
    "            pprint(self.files)    \n",
    "            print('\\n')\n",
    "\n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "        filename, if not None, should not be full name. use import ntpath ntpath.basename(\"a\\b\\c\")\n",
    "    \"\"\"\n",
    "    def SaveToCsv_FromUrl(self,url): # tab delimiter only \n",
    "        url=url.strip() \n",
    "        content=self.Read_Oneurl(url) #in format of (a,a,a,a)   \n",
    "        self.SaveToCsv(url, content)\n",
    "        #return SaveToCsv_FromResponse()\n",
    "    \n",
    "    \n",
    "    def setMaxpage(self, maxPages):\n",
    "        CollegeCrawl.max_Pages = maxPages\n",
    "        \n",
    "    def setGap(self, gap):\n",
    "        CollegeCrawl.gap_Insecond = gap\n",
    "    \n",
    "    \n",
    "    def crawl(self):\n",
    "        self.getAllUrls()\n",
    "        self.save_Summaries()\n",
    "        \n",
    "     \n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************Crawl UCLA(http://www.ucla.edu)************\n",
      "Summary of status code for URLs so far\n",
      "   status_code  url\n",
      "0            0   65\n",
      "1          200   10\n",
      "\n",
      "\n",
      "Visited URLs so far, including previously visited URLs\n",
      "['http://www.ucla.edu', 'http://www.ucla.edu/admission', 'http://www.ucla.edu/admission/undergraduate-admission', 'http://www.ucla.edu/admission/graduate-admission', 'http://www.ucla.edu/admission/extension-enrollment', 'http://www.ucla.edu/admission/international-admission', 'http://www.ucla.edu/admission/transfer-admission', 'http://www.ucla.edu/admission/affordability', 'http://www.ucla.edu/visit', 'http://www.ucla.edu/apply']\n",
      "\n",
      "The following 5 file(s) are generated this run. \n",
      "['C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\http___www_dot_ucla_dot_edu_admission_graduate-admission_01_02_2020_19_36_11.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\http___www_dot_ucla_dot_edu_admission_extension-enrollment_01_02_2020_19_36_11.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\http___www_dot_ucla_dot_edu_admission_international-admission_01_02_2020_19_36_12.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\http___www_dot_ucla_dot_edu_admission_undergraduate-admission_01_02_2020_19_36_12.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\http___www_dot_ucla_dot_edu_visit_01_02_2020_19_36_12.csv']\n",
      "\n",
      "\n",
      "************Crawl Stanford(https://www.stanford.edu/)************\n",
      "Summary of status code for URLs so far\n",
      "   status_code  url\n",
      "0            0   15\n",
      "1          200   10\n",
      "\n",
      "\n",
      "Visited URLs so far, including previously visited URLs\n",
      "['https://www.stanford.edu/', 'https://www.stanford.edu', 'https://www.stanford.edu/news', 'https://www.stanford.edu/events', 'https://www.stanford.edu/admission', 'https://www.stanford.edu/about', 'https://www.stanford.edu/atoz', 'https://www.stanford.edu/contact', 'https://www.stanford.edu/search', 'https://www.stanford.edu/site']\n",
      "\n",
      "The following 5 file(s) are generated this run. \n",
      "['C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_stanford_dot_edu_about_01_02_2020_19_36_13.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_stanford_dot_edu_search_01_02_2020_19_36_14.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_stanford_dot_edu_events_01_02_2020_19_36_14.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_stanford_dot_edu_contact_01_02_2020_19_36_15.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_stanford_dot_edu_site_01_02_2020_19_36_16.csv']\n",
      "\n",
      "\n",
      "************Crawl Yale(https://www.yale.edu/)************\n",
      "Summary of status code for URLs so far\n",
      "   status_code  url\n",
      "0            0   23\n",
      "1          200   10\n",
      "\n",
      "\n",
      "Visited URLs so far, including previously visited URLs\n",
      "['https://www.yale.edu/', 'https://www.yale.edu/calendars', 'https://www.yale.edu/search', 'https://www.yale.edu/academics', 'https://www.yale.edu/admissions', 'https://www.yale.edu/admissions/graduate-professional-schools', 'https://www.yale.edu/admissions/financial-aid', 'https://www.yale.edu/admissions/non-degree-seeking-students', 'https://www.yale.edu/life-yale', 'https://www.yale.edu/faculty']\n",
      "\n",
      "The following 5 file(s) are generated this run. \n",
      "['C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_yale_dot_edu_search_01_02_2020_19_36_16.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_yale_dot_edu_faculty_01_02_2020_19_36_16.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_yale_dot_edu_life-yale_01_02_2020_19_36_16.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_yale_dot_edu_academics_01_02_2020_19_36_17.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_yale_dot_edu_calendars_01_02_2020_19_36_17.csv']\n",
      "\n",
      "\n",
      "************Crawl UW(http://www.washington.edu/)************\n",
      "Summary of status code for URLs so far\n",
      "   status_code  url\n",
      "0            0   84\n",
      "1          200   10\n",
      "\n",
      "\n",
      "Visited URLs so far, including previously visited URLs\n",
      "['http://www.washington.edu/', 'http://www.washington.edu', 'http://www.washington.edu/boundless', 'http://www.washington.edu/jobs', 'http://www.washington.edu/safety', 'http://www.washington.edu/rules/wac', 'https://www.washington.edu/safety', 'https://www.washington.edu/rules', 'https://www.washington.edu/rules/wac/admission-and-registration-procedures', 'https://www.washington.edu/rules/wac']\n",
      "\n",
      "The following 5 file(s) are generated this run. \n",
      "['C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\http___www_dot_washington_dot_edu_rules_wac_01_02_2020_19_36_17.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_washington_dot_edu_rules_wac_admission-and-registration-procedures_01_02_2020_19_36_17.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_washington_dot_edu_rules_01_02_2020_19_36_18.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\http___www_dot_washington_dot_edu_boundless_01_02_2020_19_36_18.csv',\n",
      " 'C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2019dec29\\\\collegeFiles\\\\https___www_dot_washington_dot_edu_rules_wac_01_02_2020_19_36_18.csv']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colleges = [[\"UCLA\", \"http://www.ucla.edu\"], \n",
    "           [\"Stanford\", \"https://www.stanford.edu/\"],\n",
    "           [\"Yale\", \"https://www.yale.edu/\"],\n",
    "           [\"UW\", \"http://www.washington.edu/\"]] \n",
    "for college in colleges:\n",
    "    cr = CollegeCrawl(college[0], college[1])\n",
    "    cr.crawl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kentt\\stempro\\hw\\y2019dec29\\collegeFiles\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url)\n",
    "    #response = requests.get(url,None, headers=headers)\n",
    "    print(\"response status code for URL {} is {}\".format(url,response.status_code))\n",
    "    print(\"request headers for URL {} is {}\".format(url, response.request.headers))\n",
    "    \n",
    "    \n",
    "    #help(response)\n",
    "    #help(requests.get)\n",
    "    \n",
    "    print(response.text)\n",
    " \n",
    "\n",
    "\n",
    "getResponse(\"https://www.mit.edu\")\n",
    "#getResponseCode(\"http://www.stanford.edu\")\n",
    "#getResponseCode(\"http://www.harvard.edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ih[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kentt\\stempro\\hw\\y2019dec29\\collegeFiles\n"
     ]
    }
   ],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
