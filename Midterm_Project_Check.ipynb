{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import spacy  \n",
    "from spacy import displacy \n",
    "import en_core_web_sm \n",
    "from pathlib import Path \n",
    "from IPython.display import display, Markdown, Latex\n",
    "import sys \n",
    "import csv \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from os import path \n",
    "from datetime import datetime \n",
    "from urllib.parse import urlparse, urljoin \n",
    "import requests \n",
    "import ntpath \n",
    "from bs4 import BeautifulSoup, Comment \n",
    "import time \n",
    "from pprint import pprint  \n",
    "import shutil \n",
    "import tldextract \n",
    "import multiprocessing \n",
    "from multiprocessing import Process, Queue, cpu_count \n",
    "import os \n",
    "from os import path, listdir\n",
    "###\n",
    "from IPython.core.debugger import set_trace\n",
    "import random\n",
    "from fractions import Fraction\n",
    "import nltk\n",
    "import pprint as pp\n",
    "from collections import Counter\n",
    "from itertools import repeat, chain\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T21:36:19.317874Z",
     "start_time": "2020-01-19T21:36:19.309877Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Midterm Project Learning Status Check\n",
       "## <font color=\"red\">Due: Feb 1, 2020</font>\n",
       "## Open Book Midterm:\n",
       "<ol>\n",
       "    <li><b>(200 Points)Project Statement</b><br>: What are we doing? and in high level, how we are doing it <br>You may review Phase I/II PowerPoints.<br>Words limit: 500 \n",
       "    <br></li>\n",
       "    <br><li><b>(200 Points)Impersonate and Search</b><br>:Sign in US News Compass and conduct 3 to 5 searches. <br> Save the results to HTML files (one file per search) \n",
       "    <br><a href=\"https://premium.usnews.com/best-colleges/myfit?gpa=4.0&mode=table&sat-composite=1580\">Compass Search</a>\n",
       "    <br>A sample file from search may look like this: <br>\n",
       "    <a href='./img/OneSampleCompassHtml.html'> One Sample file</a>\n",
       "    </li>\n",
       "    <br><li><b>(800 Points)Python and Content</b><br>Design and Implement a Class to extrat result from pages generated \n",
       "    from previous question. <br>Ideal output format should reflect at the following information:  </li>\n",
       "    <br> Search Criteria: <br><img src=\"./img/CompassSearchFields.PNG\" width=\"500\"/>\n",
       "    <br> Search Results: <br><img src=\"./img/CompassSearchResults.PNG\" width=\"500\"/></li>\n",
       "    <br><li><b>(400 Points)Scratching NLTK</b><br>Crawl the following page<br>\n",
       "    <a href=\"https://www.nme.com/features/greta-thunberg-full-speech-to-the-un-2019-climate-change-summit-2550824\">Greta Thunberg UN Speech</a></li>\n",
       "    <br> Tasks: \n",
       "    <br><ul>\n",
       "        <li>Get the content of her speech</li>\n",
       "        <li>Build a dataset that list all words in this speech and how many times each word appears</li>\n",
       "        <li>Sort the list of words by <br>a).Alphabetically. <br> b). by frequency\n",
       "        <li>Lemmatize all words</li>\n",
       "        <li>Choose ~30 \"difficult\" words and get the synonyms and antoyms for them.</li> \n",
       "    </ul>\n",
       "    </li>\n",
       "</ol>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Image\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "# Midterm Project Learning Status Check\n",
    "## <font color=\"red\">Due: Feb 1, 2020</font>\n",
    "## Open Book Midterm:\n",
    "<ol>\n",
    "    <li><b>(200 Points)Project Statement</b><br>: What are we doing? and in high level, how we are doing it <br>You may review Phase I/II PowerPoints.<br>Words limit: 500 \n",
    "    <br></li>\n",
    "    <br><li><b>(200 Points)Impersonate and Search</b><br>:Sign in US News Compass and conduct 3 to 5 searches. <br> Save the results to HTML files (one file per search) \n",
    "    <br><a href=\"https://premium.usnews.com/best-colleges/myfit?gpa=4.0&mode=table&sat-composite=1580\">Compass Search</a>\n",
    "    <br>A sample file from search may look like this: <br>\n",
    "    <a href='./img/OneSampleCompassHtml.html'> One Sample file</a>\n",
    "    </li>\n",
    "    <br><li><b>(800 Points)Python and Content</b><br>Design and Implement a Class to extrat result from pages generated \n",
    "    from previous question. <br>Ideal output format should reflect at the following information:  </li>\n",
    "    <br> Search Criteria: <br><img src=\"./img/CompassSearchFields.PNG\" width=\"500\"/>\n",
    "    <br> Search Results: <br><img src=\"./img/CompassSearchResults.PNG\" width=\"500\"/></li>\n",
    "    <br><li><b>(400 Points)Scratching NLTK</b><br>Crawl the following page<br>\n",
    "    <a href=\"https://www.nme.com/features/greta-thunberg-full-speech-to-the-un-2019-climate-change-summit-2550824\">Greta Thunberg UN Speech</a></li>\n",
    "    <br> Tasks: \n",
    "    <br><ul>\n",
    "        <li>Get the content of her speech</li>\n",
    "        <li>Build a dataset that list all words in this speech and how many times each word appears</li>\n",
    "        <li>Sort the list of words by <br>a).Alphabetically. <br> b). by frequency\n",
    "        <li>Lemmatize all words</li>\n",
    "        <li>Choose ~30 \"difficult\" words and get the synonyms and antoyms for them.</li> \n",
    "    </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\"\"\"\n",
    "))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T20:13:22.503472Z",
     "start_time": "2020-01-19T20:13:22.497469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# 1 Project Management\n",
       "<body>\n",
       "    <b>What are we doing? </b> \n",
       "    <br/>\n",
       "    <p>\n",
       "      What we are doing is to help the students to find the best colleges based on their info. \n",
       "    </p>\n",
       "    <br/>  \n",
       "    <b>How are we doing it?</b>\n",
       "    <br/>\n",
       "    <p>\n",
       "      We provide a React application as UI that can be accessed in a web brower.<br/>\n",
       "      Students can do two things in our app: \n",
       "      (1) students can enter their information like GPA, SAT/ACT score etc or other search criteria like\n",
       "      desired school location or tuition range, and our application will give a list of recommended colleges.  \n",
       "      (2) students can enter their information like GPA and their desired college, and our app will do a gap\n",
       "      analysis to give suggestions of actions to be taken in order to get into that desired college.<br/>\n",
       "      As mentioned the UI front end is using react and the backend is handled through node.js. The college \n",
       "      data will be gathered by web crawling, and AI and python is used to analyze the input and data \n",
       "      to generate output.     \n",
       "   </P>  \n",
       "</body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Image\n",
    "display(Markdown( \n",
    "\"\"\"\n",
    "# 1 Project Management\n",
    "<body>\n",
    "    <b>What are we doing? </b> \n",
    "    <br/>\n",
    "    <p>\n",
    "      What we are doing is to help the students to find the best colleges based on their info. \n",
    "    </p>\n",
    "    <br/>  \n",
    "    <b>How are we doing it?</b>\n",
    "    <br/>\n",
    "    <p>\n",
    "      We provide a React application as UI that can be accessed in a web brower.<br/>\n",
    "      Students can do two things in our app: \n",
    "      (1) students can enter their information like GPA, SAT/ACT score etc or other search criteria like\n",
    "      desired school location or tuition range, and our application will give a list of recommended colleges.  \n",
    "      (2) students can enter their information like GPA and their desired college, and our app will do a gap\n",
    "      analysis to give suggestions of actions to be taken in order to get into that desired college.<br/>\n",
    "      As mentioned the UI front end is using react and the backend is handled through node.js. The college \n",
    "      data will be gathered by web crawling, and AI and python is used to analyze the input and data \n",
    "      to generate output.     \n",
    "   </P>  \n",
    "</body>\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Check github for files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Fit College Search\n",
      "\n",
      "        Criteria             Values\n",
      "             GPA                  1\n",
      "   SAT Composite                400\n",
      "             ACT                 10\n",
      " Intended Majors   Computer Science\n",
      "My Fit                   School Name SAT Math SAT Reading  ACT Avg. High School GPA\n",
      "    46  Providence Christian College      N/A         N/A  N/A                  N/A\n",
      "My Fit College Search\n",
      "\n",
      "      Criteria Values\n",
      "           GPA    2.4\n",
      " SAT Composite   1000\n",
      "           ACT     25\n",
      "My Fit                                        School Name SAT Math SAT Reading  ACT Avg. High School GPA\n",
      "    82                                     Fisher College      N/A         N/A  N/A                  2.3\n",
      "    80                           Clayton State University      446         489   20                  2.4\n",
      "    79                                New England College      470         440  N/A                  2.5\n",
      "    77                         St. Augustine's University      N/A         N/A  N/A                  2.4\n",
      "    76                           Jarvis Christian College      472         400   15                  2.4\n",
      "    75                                Livingstone College      421         410   16                  2.5\n",
      "    73                                    Shaw University      420         436   15                  2.5\n",
      "    71                                  Chowan University      442         460   16                  2.7\n",
      "    71                                  Limestone College      487         497   19                  2.5\n",
      "    71                          Kentucky State University      455         482   18                  2.7\n",
      "    71                      Calumet College of St. Joseph      462         468   18                  2.7\n",
      "    71                           Central State University      391         407   17                  2.7\n",
      "    70                University of Wisconsin--Whitewater      N/A         N/A  N/A                  3.3\n",
      "    70                                 Lincoln University      445         451   17                  2.7\n",
      "    70                                  SUNY--Morrisville      524         511   20                  2.6\n",
      "    68                                     Monroe College      471         477  N/A                  2.7\n",
      "    66                              Holy Names University      500         310   19                  N/A\n",
      "    66                        Johnson C. Smith University      444         460   17                  2.8\n",
      "    66                    University of Louisiana--Monroe      N/A         N/A   23                  3.3\n",
      "    65                      Southern Adventist University      420         460   23                  3.5\n",
      "    65  SUNY College of Agriculture and Technology--Co...      492         502   20                  2.8\n",
      "    65                          Virginia State University      454         476   18                  2.9\n",
      "    65                           Robert Morris University      N/A         N/A   18                  2.8\n",
      "    65                    Elizabeth City State University      480         491   18                  3.2\n",
      "    65                     Concordia University--St. Paul      N/A         N/A   21                  3.2\n",
      "My Fit College Search\n",
      "\n",
      "         Criteria           Values\n",
      "              GPA              3.5\n",
      "    SAT Composite             1100\n",
      "              ACT               22\n",
      "       Enrollment        0 - 6,440\n",
      " Tuition and Fees   < $0 - $26,000\n",
      "My Fit                                   School Name SAT Math SAT Reading  ACT Avg. High School GPA\n",
      "    87   California State University--San Bernardino      N/A         N/A  N/A                  3.3\n",
      "    87       California State University--Stanislaus      495         507   19                  3.3\n",
      "    85           California State University--Fresno      506         518   19                  3.5\n",
      "    85                 Mount Saint Mary's University      500         520   19                  3.4\n",
      "    84      California State University--Los Angeles      492         496   18                  3.2\n",
      "    84                      Saint Peter's University      514         519   20                  3.3\n",
      "    84                   Eastern Illinois University      507         525   21                  3.2\n",
      "    84                              Wesleyan College      494         531   20                  3.4\n",
      "    83                               Kean University      495         497   20                  3.0\n",
      "    83                            Neumann University      498         506   19                  3.1\n",
      "    83             North Carolina Central University      480         490   18                  3.2\n",
      "    83         California State University--East Bay      495         499   18                  3.2\n",
      "    83                           Felician University      498         500   18                  3.2\n",
      "    83                           Caldwell University      530         535   20                  3.3\n",
      "    83                            Claflin University      470         480   19                  3.2\n",
      "    83                      Robert Morris University      N/A         N/A   18                  2.8\n",
      "    83  California State University--Dominguez Hills      473         482   17                  3.2\n",
      "    83                   Cardinal Stritch University      494         488   20                  3.1\n",
      "    83                             The Sage Colleges      517         521   20                  3.2\n",
      "    83             Columbia International University      511         532   21                  3.4\n",
      "    82                      Gwynedd Mercy University      512         517   21                  3.2\n",
      "    82                            Lincoln University      466         476   18                  2.9\n",
      "    82                         Chestnut Hill College      513         522   18                  3.1\n",
      "    82       California State University--Northridge      520         520   19                  3.4\n",
      "    82                          Gallaudet University      N/A         N/A   18                  3.2\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "class CollegeData():\n",
    "    \n",
    "    \n",
    "    def __init__(self, htmlFile='C:\\\\Users\\\\kentt\\\\stempro\\\\hw\\\\midterms\\\\data\\\\college3.html'):\n",
    "        htmlPath = Path(htmlFile) \n",
    "        self.criteriaDF = pd.DataFrame()\n",
    "        self.resultDF = pd.DataFrame()\n",
    "        self.resultFile = htmlPath.parent / (htmlPath.stem +\".csv\")\n",
    "        self.htmlFile = htmlFile\n",
    "    \n",
    "    def run(self):\n",
    "        soup=BeautifulSoup(open(self.htmlFile))\n",
    "        self.getCriteria(soup)\n",
    "        self.getResult(soup)\n",
    "        self.saveToFile()\n",
    "    \n",
    "    def getCriteria(self, soup):\n",
    "   \n",
    "    # get search criteria\n",
    "    # search criteria is a dictionary like {'GPA': ' 3.5', 'SAT Composite': ' 1100', 'ACT': ' 22'', 'Tuition and Fees': ' < $0 - $26,000'}\n",
    "        criteria={}\n",
    "        for item in soup.find_all('button'):\n",
    "            for x in item.find_all(\"span\"):\n",
    "                query = x.text.strip()\n",
    "                a = query.split(':')\n",
    "                criteria[a[0]]=a[1]\n",
    "        print(\"My Fit College Search\\n\")\n",
    "        self.criteriaDF = pd.DataFrame(criteria.items(), columns=[\"Criteria\", \"Values\"])\n",
    "        print(self.criteriaDF.to_string(index=False))\n",
    "       # criteriaDF.to_csv('file3search.csv',index=False)\n",
    "        \n",
    "    \n",
    "    def getResult(self, soup):\n",
    "    # get search result\n",
    "    # search result is a list(table) with each element also a list(tr) and \n",
    "    # each element (td) of the inner list is each field\n",
    "    # e.g [['87', 'California State University--San Bernardino', 'N/A', 'N/A', 'N/A', '3.3'], ['87', 'California State University--Stanislaus', '495', '507', '19', '3.3'], ['85', 'California State University--Fresno', '506', '518', '19', '3.5']]\n",
    "        result = []\n",
    "        for item1 in soup.find_all('table'):\n",
    "            for tr in item1.find_all(\"tr\"):\n",
    "                row =[]\n",
    "                for cell in tr.find_all([\"th\",\"td\"]):\n",
    "                    if cell.name == \"th\":\n",
    "                        row.append(cell.text.strip())\n",
    "                    elif cell.name == \"td\":\n",
    "                        for div in cell.find_all(\"div\", {\"class\":\"show-for-medium-up\"}):\n",
    "                            texts = div.find_all(text=True)\n",
    "                            text2 = filter(lambda text: text.strip(), texts)#remove empty string or \\n\n",
    "                            validTexts = list(map(lambda x: re.sub(r'[\\n ] ','',x), text2))#\n",
    "                            row.append(validTexts[0])\n",
    "                result.append(row)\n",
    "            \n",
    "        headers = result.pop(0)\n",
    "        self.resultDF = pd.DataFrame(result, columns=headers)\n",
    "        print(self.resultDF.to_string(index=False))\n",
    "        #resultDF.to_csv('file3.csv',index=False)\n",
    "    \n",
    "    \n",
    "    def saveToFile(self):\n",
    "        #first write criteria, then write the search result\n",
    "        self.criteriaDF.to_csv(self.resultFile, index=False, mode='w')\n",
    "        self.resultDF.to_csv(self.resultFile, index=False, mode='a')\n",
    "        \n",
    "files = ['C:\\\\Users\\\\kentt\\\\stempro\\\\hw\\\\midterms\\\\data\\\\college3.html','C:\\\\Users\\\\kentt\\\\stempro\\\\hw\\\\midterms\\\\data\\\\college2.html','C:\\\\Users\\\\kentt\\\\stempro\\\\hw\\\\midterms\\\\data\\\\college1.html']        \n",
    "for file in files:\n",
    "    cd=CollegeData(file)\n",
    "    cd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words sorted by frequency\n",
      "*************************************************\n",
      "Counter({'you': 22,\n",
      "         'the': 19,\n",
      "         'and': 17,\n",
      "         'of': 14,\n",
      "         'to': 13,\n",
      "         'are': 11,\n",
      "         'is': 8,\n",
      "         'that': 8,\n",
      "         'be': 8,\n",
      "         'in': 7,\n",
      "         'with': 7,\n",
      "         'i': 6,\n",
      "         'us': 6,\n",
      "         'not': 6,\n",
      "         'all': 5,\n",
      "         'how': 5,\n",
      "         'we': 5,\n",
      "         'a': 5,\n",
      "         'my': 4,\n",
      "         'here': 4,\n",
      "         'on': 4,\n",
      "         'people': 4,\n",
      "         'dare': 4,\n",
      "         'will': 4,\n",
      "         'this': 3,\n",
      "         'for': 3,\n",
      "         'have': 3,\n",
      "         'your': 3,\n",
      "         'than': 3,\n",
      "         'years': 3,\n",
      "         'say': 3,\n",
      "         'solutions': 3,\n",
      "         'but': 3,\n",
      "         '50%': 3,\n",
      "         'co2': 3,\n",
      "         'it': 3,\n",
      "         'up': 2,\n",
      "         'back': 2,\n",
      "         'yet': 2,\n",
      "         'come': 2,\n",
      "         'young': 2,\n",
      "         'can': 2,\n",
      "         'has': 2,\n",
      "         'away': 2,\n",
      "         'enough': 2,\n",
      "         'understand': 2,\n",
      "         'believe': 2,\n",
      "         'because': 2,\n",
      "         'if': 2,\n",
      "         'still': 2,\n",
      "         'failing': 2,\n",
      "         'chance': 2,\n",
      "         'staying': 2,\n",
      "         'below': 2,\n",
      "         '15°c': 2,\n",
      "         'acceptable': 2,\n",
      "         'numbers': 2,\n",
      "         'by': 2,\n",
      "         'air': 2,\n",
      "         'world': 2,\n",
      "         'gigatons': 2,\n",
      "         'today': 2,\n",
      "         'less': 2,\n",
      "         'or': 2,\n",
      "         'line': 2,\n",
      "         'these': 2,\n",
      "         'like': 2,\n",
      "         'right': 2,\n",
      "         'message': 1,\n",
      "         'we’ll': 1,\n",
      "         'watching': 1,\n",
      "         'wrong': 1,\n",
      "         'shouldn’t': 1,\n",
      "         'should': 1,\n",
      "         'school': 1,\n",
      "         'other': 1,\n",
      "         'side': 1,\n",
      "         'ocean': 1,\n",
      "         'hope': 1,\n",
      "         'stolen': 1,\n",
      "         'dreams': 1,\n",
      "         'childhood': 1,\n",
      "         'empty': 1,\n",
      "         'words': 1,\n",
      "         'i’m': 1,\n",
      "         'one': 1,\n",
      "         'lucky': 1,\n",
      "         'ones': 1,\n",
      "         'suffering': 1,\n",
      "         'dying': 1,\n",
      "         'entire': 1,\n",
      "         'ecosystems': 1,\n",
      "         'collapsing': 1,\n",
      "         'beginning': 1,\n",
      "         'mass': 1,\n",
      "         'extinction': 1,\n",
      "         'talk': 1,\n",
      "         'about': 1,\n",
      "         'money': 1,\n",
      "         'fairy': 1,\n",
      "         'tales': 1,\n",
      "         'eternal': 1,\n",
      "         'economic': 1,\n",
      "         'growth': 1,\n",
      "         'more': 1,\n",
      "         '30': 1,\n",
      "         'science': 1,\n",
      "         'been': 1,\n",
      "         'crystal': 1,\n",
      "         'clear': 1,\n",
      "         'continue': 1,\n",
      "         'look': 1,\n",
      "         'doing': 1,\n",
      "         'when': 1,\n",
      "         'politics': 1,\n",
      "         'needed': 1,\n",
      "         'nowhere': 1,\n",
      "         'sight': 1,\n",
      "         'hear': 1,\n",
      "         'urgency': 1,\n",
      "         'no': 1,\n",
      "         'matter': 1,\n",
      "         'sad': 1,\n",
      "         'angry': 1,\n",
      "         'am': 1,\n",
      "         'don’t': 1,\n",
      "         'want': 1,\n",
      "         'really': 1,\n",
      "         'understood': 1,\n",
      "         'situation': 1,\n",
      "         'kept': 1,\n",
      "         'act': 1,\n",
      "         'then': 1,\n",
      "         'would': 1,\n",
      "         'evil': 1,\n",
      "         'refuse': 1,\n",
      "         'popular': 1,\n",
      "         'idea': 1,\n",
      "         'cutting': 1,\n",
      "         'our': 1,\n",
      "         'emissions': 1,\n",
      "         'half': 1,\n",
      "         '10': 1,\n",
      "         'only': 1,\n",
      "         'gives': 1,\n",
      "         'risks': 1,\n",
      "         'setting': 1,\n",
      "         'off': 1,\n",
      "         'irreversible': 1,\n",
      "         'chain': 1,\n",
      "         'reactions': 1,\n",
      "         'beyond': 1,\n",
      "         'human': 1,\n",
      "         'control': 1,\n",
      "         'may': 1,\n",
      "         'those': 1,\n",
      "         'do': 1,\n",
      "         'include': 1,\n",
      "         'tipping': 1,\n",
      "         'points': 1,\n",
      "         'most': 1,\n",
      "         'feedback': 1,\n",
      "         'loops': 1,\n",
      "         'additional': 1,\n",
      "         'warming': 1,\n",
      "         'hidden': 1,\n",
      "         'toxic': 1,\n",
      "         'pollution': 1,\n",
      "         'aspects': 1,\n",
      "         'equity': 1,\n",
      "         'climate': 1,\n",
      "         'justice': 1,\n",
      "         'they': 1,\n",
      "         'also': 1,\n",
      "         'rely': 1,\n",
      "         'generation': 1,\n",
      "         'sucking': 1,\n",
      "         'hundreds': 1,\n",
      "         'billions': 1,\n",
      "         'tons': 1,\n",
      "         'out': 1,\n",
      "         'technologies': 1,\n",
      "         'barely': 1,\n",
      "         'exist': 1,\n",
      "         'so': 1,\n",
      "         'risk': 1,\n",
      "         'simply': 1,\n",
      "         'who': 1,\n",
      "         'live': 1,\n",
      "         'consequences': 1,\n",
      "         '67%': 1,\n",
      "         'global': 1,\n",
      "         'temperature': 1,\n",
      "         'rise': 1,\n",
      "         'best': 1,\n",
      "         'odds': 1,\n",
      "         'given': 1,\n",
      "         'ipcc': 1,\n",
      "         'had': 1,\n",
      "         '420': 1,\n",
      "         'emit': 1,\n",
      "         'january': 1,\n",
      "         '1st': 1,\n",
      "         '2018': 1,\n",
      "         'figure': 1,\n",
      "         'already': 1,\n",
      "         'down': 1,\n",
      "         '350': 1,\n",
      "         'pretend': 1,\n",
      "         'solved': 1,\n",
      "         'business': 1,\n",
      "         'as': 1,\n",
      "         'usual': 1,\n",
      "         'some': 1,\n",
      "         'technical': 1,\n",
      "         'today’s': 1,\n",
      "         'emission': 1,\n",
      "         'levels': 1,\n",
      "         'remaining': 1,\n",
      "         'budget': 1,\n",
      "         'entirely': 1,\n",
      "         'gone': 1,\n",
      "         'eight-and-a0-half': 1,\n",
      "         'there': 1,\n",
      "         'any': 1,\n",
      "         'plans': 1,\n",
      "         'presented': 1,\n",
      "         'figures': 1,\n",
      "         'too': 1,\n",
      "         'uncomfortable': 1,\n",
      "         'mature': 1,\n",
      "         'tell': 1,\n",
      "         'starting': 1,\n",
      "         'betrayal': 1,\n",
      "         'eyes': 1,\n",
      "         'future': 1,\n",
      "         'generations': 1,\n",
      "         'upon': 1,\n",
      "         'choose': 1,\n",
      "         'fail': 1,\n",
      "         'never': 1,\n",
      "         'forgive': 1,\n",
      "         'let': 1,\n",
      "         'get': 1,\n",
      "         'now': 1,\n",
      "         'where': 1,\n",
      "         'draw': 1,\n",
      "         'waking': 1,\n",
      "         'change': 1,\n",
      "         'coming': 1,\n",
      "         'whether': 1,\n",
      "         'thank': 1})\n",
      "\n",
      "\n",
      "List of words sorted alphabetically\n",
      "*************************************************\n",
      "['10',\n",
      " '15°c',\n",
      " '15°c',\n",
      " '1st',\n",
      " '2018',\n",
      " '30',\n",
      " '350',\n",
      " '420',\n",
      " '50%',\n",
      " '50%',\n",
      " '50%',\n",
      " '67%',\n",
      " 'a',\n",
      " 'a',\n",
      " 'a',\n",
      " 'a',\n",
      " 'a',\n",
      " 'about',\n",
      " 'acceptable',\n",
      " 'acceptable',\n",
      " 'act',\n",
      " 'additional',\n",
      " 'air',\n",
      " 'air',\n",
      " 'all',\n",
      " 'all',\n",
      " 'all',\n",
      " 'all',\n",
      " 'all',\n",
      " 'already',\n",
      " 'also',\n",
      " 'am',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'and',\n",
      " 'angry',\n",
      " 'any',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'are',\n",
      " 'as',\n",
      " 'aspects',\n",
      " 'away',\n",
      " 'away',\n",
      " 'back',\n",
      " 'back',\n",
      " 'barely',\n",
      " 'be',\n",
      " 'be',\n",
      " 'be',\n",
      " 'be',\n",
      " 'be',\n",
      " 'be',\n",
      " 'be',\n",
      " 'be',\n",
      " 'because',\n",
      " 'because',\n",
      " 'been',\n",
      " 'beginning',\n",
      " 'believe',\n",
      " 'believe',\n",
      " 'below',\n",
      " 'below',\n",
      " 'best',\n",
      " 'betrayal',\n",
      " 'beyond',\n",
      " 'billions',\n",
      " 'budget',\n",
      " 'business',\n",
      " 'but',\n",
      " 'but',\n",
      " 'but',\n",
      " 'by',\n",
      " 'by',\n",
      " 'can',\n",
      " 'can',\n",
      " 'chain',\n",
      " 'chance',\n",
      " 'chance',\n",
      " 'change',\n",
      " 'childhood',\n",
      " 'choose',\n",
      " 'clear',\n",
      " 'climate',\n",
      " 'co2',\n",
      " 'co2',\n",
      " 'co2',\n",
      " 'collapsing',\n",
      " 'come',\n",
      " 'come',\n",
      " 'coming',\n",
      " 'consequences',\n",
      " 'continue',\n",
      " 'control',\n",
      " 'crystal',\n",
      " 'cutting',\n",
      " 'dare',\n",
      " 'dare',\n",
      " 'dare',\n",
      " 'dare',\n",
      " 'do',\n",
      " 'doing',\n",
      " 'don’t',\n",
      " 'down',\n",
      " 'draw',\n",
      " 'dreams',\n",
      " 'dying',\n",
      " 'economic',\n",
      " 'ecosystems',\n",
      " 'eight-and-a0-half',\n",
      " 'emission',\n",
      " 'emissions',\n",
      " 'emit',\n",
      " 'empty',\n",
      " 'enough',\n",
      " 'enough',\n",
      " 'entire',\n",
      " 'entirely',\n",
      " 'equity',\n",
      " 'eternal',\n",
      " 'evil',\n",
      " 'exist',\n",
      " 'extinction',\n",
      " 'eyes',\n",
      " 'fail',\n",
      " 'failing',\n",
      " 'failing',\n",
      " 'fairy',\n",
      " 'feedback',\n",
      " 'figure',\n",
      " 'figures',\n",
      " 'for',\n",
      " 'for',\n",
      " 'for',\n",
      " 'forgive',\n",
      " 'future',\n",
      " 'generation',\n",
      " 'generations',\n",
      " 'get',\n",
      " 'gigatons',\n",
      " 'gigatons',\n",
      " 'given',\n",
      " 'gives',\n",
      " 'global',\n",
      " 'gone',\n",
      " 'growth',\n",
      " 'had',\n",
      " 'half',\n",
      " 'has',\n",
      " 'has',\n",
      " 'have',\n",
      " 'have',\n",
      " 'have',\n",
      " 'hear',\n",
      " 'here',\n",
      " 'here',\n",
      " 'here',\n",
      " 'here',\n",
      " 'hidden',\n",
      " 'hope',\n",
      " 'how',\n",
      " 'how',\n",
      " 'how',\n",
      " 'how',\n",
      " 'how',\n",
      " 'human',\n",
      " 'hundreds',\n",
      " 'i',\n",
      " 'i',\n",
      " 'i',\n",
      " 'i',\n",
      " 'i',\n",
      " 'i',\n",
      " 'idea',\n",
      " 'if',\n",
      " 'if',\n",
      " 'in',\n",
      " 'in',\n",
      " 'in',\n",
      " 'in',\n",
      " 'in',\n",
      " 'in',\n",
      " 'in',\n",
      " 'include',\n",
      " 'ipcc',\n",
      " 'irreversible',\n",
      " 'is',\n",
      " 'is',\n",
      " 'is',\n",
      " 'is',\n",
      " 'is',\n",
      " 'is',\n",
      " 'is',\n",
      " 'is',\n",
      " 'it',\n",
      " 'it',\n",
      " 'it',\n",
      " 'i’m',\n",
      " 'january',\n",
      " 'justice',\n",
      " 'kept',\n",
      " 'less',\n",
      " 'less',\n",
      " 'let',\n",
      " 'levels',\n",
      " 'like',\n",
      " 'like',\n",
      " 'line',\n",
      " 'line',\n",
      " 'live',\n",
      " 'look',\n",
      " 'loops',\n",
      " 'lucky',\n",
      " 'mass',\n",
      " 'matter',\n",
      " 'mature',\n",
      " 'may',\n",
      " 'message',\n",
      " 'money',\n",
      " 'more',\n",
      " 'most',\n",
      " 'my',\n",
      " 'my',\n",
      " 'my',\n",
      " 'my',\n",
      " 'needed',\n",
      " 'never',\n",
      " 'no',\n",
      " 'not',\n",
      " 'not',\n",
      " 'not',\n",
      " 'not',\n",
      " 'not',\n",
      " 'not',\n",
      " 'now',\n",
      " 'nowhere',\n",
      " 'numbers',\n",
      " 'numbers',\n",
      " 'ocean',\n",
      " 'odds',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'of',\n",
      " 'off',\n",
      " 'on',\n",
      " 'on',\n",
      " 'on',\n",
      " 'on',\n",
      " 'one',\n",
      " 'ones',\n",
      " 'only',\n",
      " 'or',\n",
      " 'or',\n",
      " 'other',\n",
      " 'our',\n",
      " 'out',\n",
      " 'people',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'people',\n",
      " 'people',\n",
      " 'people',\n",
      " 'plans',\n",
      " 'points',\n",
      " 'politics',\n",
      " 'pollution',\n",
      " 'popular',\n",
      " 'presented',\n",
      " 'pretend',\n",
      " 'reactions',\n",
      " 'really',\n",
      " 'refuse',\n",
      " 'rely',\n",
      " 'remaining',\n",
      " 'right',\n",
      " 'right',\n",
      " 'rise',\n",
      " 'risk',\n",
      " 'risks',\n",
      " 'sad',\n",
      " 'say',\n",
      " 'say',\n",
      " 'say',\n",
      " 'school',\n",
      " 'science',\n",
      " 'setting',\n",
      " 'should',\n",
      " 'shouldn’t',\n",
      " 'side',\n",
      " 'sight',\n",
      " 'simply',\n",
      " 'situation',\n",
      " 'so',\n",
      " 'solutions',\n",
      " 'solutions',\n",
      " 'solutions',\n",
      " 'solved',\n",
      " 'some',\n",
      " 'starting',\n",
      " 'staying',\n",
      " 'staying',\n",
      " 'still',\n",
      " 'still',\n",
      " 'stolen',\n",
      " 'sucking',\n",
      " 'suffering',\n",
      " 'tales',\n",
      " 'talk',\n",
      " 'technical',\n",
      " 'technologies',\n",
      " 'tell',\n",
      " 'temperature',\n",
      " 'than',\n",
      " 'than',\n",
      " 'than',\n",
      " 'thank',\n",
      " 'that',\n",
      " 'that',\n",
      " 'that',\n",
      " 'that',\n",
      " 'that',\n",
      " 'that',\n",
      " 'that',\n",
      " 'that',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'the',\n",
      " 'then',\n",
      " 'there',\n",
      " 'these',\n",
      " 'these',\n",
      " 'they',\n",
      " 'this',\n",
      " 'this',\n",
      " 'this',\n",
      " 'those',\n",
      " 'tipping',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'to',\n",
      " 'today',\n",
      " 'today',\n",
      " 'today’s',\n",
      " 'tons',\n",
      " 'too',\n",
      " 'toxic',\n",
      " 'uncomfortable',\n",
      " 'understand',\n",
      " 'understand',\n",
      " 'understood',\n",
      " 'up',\n",
      " 'up',\n",
      " 'upon',\n",
      " 'urgency',\n",
      " 'us',\n",
      " 'us',\n",
      " 'us',\n",
      " 'us',\n",
      " 'us',\n",
      " 'us',\n",
      " 'usual',\n",
      " 'waking',\n",
      " 'want',\n",
      " 'warming',\n",
      " 'watching',\n",
      " 'we',\n",
      " 'we',\n",
      " 'we',\n",
      " 'we',\n",
      " 'we',\n",
      " 'we’ll',\n",
      " 'when',\n",
      " 'where',\n",
      " 'whether',\n",
      " 'who',\n",
      " 'will',\n",
      " 'will',\n",
      " 'will',\n",
      " 'will',\n",
      " 'with',\n",
      " 'with',\n",
      " 'with',\n",
      " 'with',\n",
      " 'with',\n",
      " 'with',\n",
      " 'with',\n",
      " 'words',\n",
      " 'world',\n",
      " 'world',\n",
      " 'would',\n",
      " 'wrong',\n",
      " 'years',\n",
      " 'years',\n",
      " 'years',\n",
      " 'yet',\n",
      " 'yet',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'you',\n",
      " 'young',\n",
      " 'young',\n",
      " 'your',\n",
      " 'your',\n",
      " 'your']\n",
      "\n",
      "\n",
      "Lemmatizing all words with WordNetLemmatizer\n",
      "\n",
      "*************************************************\n",
      "Word\t\t\t\tLemma\n",
      "-------------------------------------------------\n",
      "my\t\t\t\tmy\n",
      "message\t\t\t\tmessage\n",
      "is\t\t\t\tis\n",
      "that\t\t\t\tthat\n",
      "we’ll\t\t\t\twe’ll\n",
      "be\t\t\t\tbe\n",
      "watching\t\t\twatching\n",
      "you\t\t\t\tyou\n",
      "this\t\t\t\tthis\n",
      "is\t\t\t\tis\n",
      "all\t\t\t\tall\n",
      "wrong\t\t\t\twrong\n",
      "i\t\t\t\ti\n",
      "shouldn’t\t\t\tshouldn’t\n",
      "be\t\t\t\tbe\n",
      "up\t\t\t\tup\n",
      "here\t\t\t\there\n",
      "i\t\t\t\ti\n",
      "should\t\t\t\tshould\n",
      "be\t\t\t\tbe\n",
      "back\t\t\t\tback\n",
      "in\t\t\t\tin\n",
      "school\t\t\t\tschool\n",
      "on\t\t\t\ton\n",
      "the\t\t\t\tthe\n",
      "other\t\t\t\tother\n",
      "side\t\t\t\tside\n",
      "of\t\t\t\tof\n",
      "the\t\t\t\tthe\n",
      "ocean\t\t\t\tocean\n",
      "yet\t\t\t\tyet\n",
      "you\t\t\t\tyou\n",
      "all\t\t\t\tall\n",
      "come\t\t\t\tcome\n",
      "to\t\t\t\tto\n",
      "us\t\t\t\tu\n",
      "young\t\t\t\tyoung\n",
      "people\t\t\t\tpeople\n",
      "for\t\t\t\tfor\n",
      "hope\t\t\t\thope\n",
      "how\t\t\t\thow\n",
      "dare\t\t\t\tdare\n",
      "you\t\t\t\tyou\n",
      "you\t\t\t\tyou\n",
      "have\t\t\t\thave\n",
      "stolen\t\t\t\tstolen\n",
      "my\t\t\t\tmy\n",
      "dreams\t\t\t\tdream\n",
      "and\t\t\t\tand\n",
      "my\t\t\t\tmy\n",
      "childhood\t\t\tchildhood\n",
      "with\t\t\t\twith\n",
      "your\t\t\t\tyour\n",
      "empty\t\t\t\tempty\n",
      "words\t\t\t\tword\n",
      "yet\t\t\t\tyet\n",
      "i’m\t\t\t\ti’m\n",
      "one\t\t\t\tone\n",
      "of\t\t\t\tof\n",
      "the\t\t\t\tthe\n",
      "lucky\t\t\t\tlucky\n",
      "ones\t\t\t\tone\n",
      "people\t\t\t\tpeople\n",
      "are\t\t\t\tare\n",
      "suffering\t\t\tsuffering\n",
      "people\t\t\t\tpeople\n",
      "are\t\t\t\tare\n",
      "dying\t\t\t\tdying\n",
      "entire\t\t\t\tentire\n",
      "ecosystems\t\t\tecosystem\n",
      "are\t\t\t\tare\n",
      "collapsing\t\t\tcollapsing\n",
      "we\t\t\t\twe\n",
      "are\t\t\t\tare\n",
      "in\t\t\t\tin\n",
      "the\t\t\t\tthe\n",
      "beginning\t\t\tbeginning\n",
      "of\t\t\t\tof\n",
      "a\t\t\t\ta\n",
      "mass\t\t\t\tmass\n",
      "extinction\t\t\textinction\n",
      "and\t\t\t\tand\n",
      "all\t\t\t\tall\n",
      "you\t\t\t\tyou\n",
      "can\t\t\t\tcan\n",
      "talk\t\t\t\ttalk\n",
      "about\t\t\t\tabout\n",
      "is\t\t\t\tis\n",
      "money\t\t\t\tmoney\n",
      "and\t\t\t\tand\n",
      "fairy\t\t\t\tfairy\n",
      "tales\t\t\t\ttale\n",
      "of\t\t\t\tof\n",
      "eternal\t\t\t\teternal\n",
      "economic\t\t\teconomic\n",
      "growth\t\t\t\tgrowth\n",
      "how\t\t\t\thow\n",
      "dare\t\t\t\tdare\n",
      "you\t\t\t\tyou\n",
      "for\t\t\t\tfor\n",
      "more\t\t\t\tmore\n",
      "than\t\t\t\tthan\n",
      "30\t\t\t\t30\n",
      "years\t\t\t\tyear\n",
      "the\t\t\t\tthe\n",
      "science\t\t\t\tscience\n",
      "has\t\t\t\tha\n",
      "been\t\t\t\tbeen\n",
      "crystal\t\t\t\tcrystal\n",
      "clear\t\t\t\tclear\n",
      "how\t\t\t\thow\n",
      "dare\t\t\t\tdare\n",
      "you\t\t\t\tyou\n",
      "continue\t\t\tcontinue\n",
      "to\t\t\t\tto\n",
      "look\t\t\t\tlook\n",
      "away\t\t\t\taway\n",
      "and\t\t\t\tand\n",
      "come\t\t\t\tcome\n",
      "here\t\t\t\there\n",
      "and\t\t\t\tand\n",
      "say\t\t\t\tsay\n",
      "you\t\t\t\tyou\n",
      "are\t\t\t\tare\n",
      "doing\t\t\t\tdoing\n",
      "enough\t\t\t\tenough\n",
      "when\t\t\t\twhen\n",
      "the\t\t\t\tthe\n",
      "politics\t\t\tpolitics\n",
      "and\t\t\t\tand\n",
      "solutions\t\t\tsolution\n",
      "needed\t\t\t\tneeded\n",
      "are\t\t\t\tare\n",
      "nowhere\t\t\t\tnowhere\n",
      "in\t\t\t\tin\n",
      "sight\t\t\t\tsight\n",
      "you\t\t\t\tyou\n",
      "say\t\t\t\tsay\n",
      "that\t\t\t\tthat\n",
      "you\t\t\t\tyou\n",
      "hear\t\t\t\thear\n",
      "us\t\t\t\tu\n",
      "and\t\t\t\tand\n",
      "understand\t\t\tunderstand\n",
      "the\t\t\t\tthe\n",
      "urgency\t\t\t\turgency\n",
      "but\t\t\t\tbut\n",
      "no\t\t\t\tno\n",
      "matter\t\t\t\tmatter\n",
      "how\t\t\t\thow\n",
      "sad\t\t\t\tsad\n",
      "and\t\t\t\tand\n",
      "angry\t\t\t\tangry\n",
      "i\t\t\t\ti\n",
      "am\t\t\t\tam\n",
      "i\t\t\t\ti\n",
      "don’t\t\t\t\tdon’t\n",
      "want\t\t\t\twant\n",
      "to\t\t\t\tto\n",
      "believe\t\t\t\tbelieve\n",
      "that\t\t\t\tthat\n",
      "because\t\t\t\tbecause\n",
      "if\t\t\t\tif\n",
      "you\t\t\t\tyou\n",
      "really\t\t\t\treally\n",
      "understood\t\t\tunderstood\n",
      "the\t\t\t\tthe\n",
      "situation\t\t\tsituation\n",
      "and\t\t\t\tand\n",
      "still\t\t\t\tstill\n",
      "kept\t\t\t\tkept\n",
      "on\t\t\t\ton\n",
      "failing\t\t\t\tfailing\n",
      "to\t\t\t\tto\n",
      "act\t\t\t\tact\n",
      "then\t\t\t\tthen\n",
      "you\t\t\t\tyou\n",
      "would\t\t\t\twould\n",
      "be\t\t\t\tbe\n",
      "evil\t\t\t\tevil\n",
      "and\t\t\t\tand\n",
      "that\t\t\t\tthat\n",
      "i\t\t\t\ti\n",
      "refuse\t\t\t\trefuse\n",
      "to\t\t\t\tto\n",
      "believe\t\t\t\tbelieve\n",
      "the\t\t\t\tthe\n",
      "popular\t\t\t\tpopular\n",
      "idea\t\t\t\tidea\n",
      "of\t\t\t\tof\n",
      "cutting\t\t\t\tcutting\n",
      "our\t\t\t\tour\n",
      "emissions\t\t\temission\n",
      "in\t\t\t\tin\n",
      "half\t\t\t\thalf\n",
      "in\t\t\t\tin\n",
      "10\t\t\t\t10\n",
      "years\t\t\t\tyear\n",
      "only\t\t\t\tonly\n",
      "gives\t\t\t\tgive\n",
      "us\t\t\t\tu\n",
      "a\t\t\t\ta\n",
      "50%\t\t\t\t50%\n",
      "chance\t\t\t\tchance\n",
      "of\t\t\t\tof\n",
      "staying\t\t\t\tstaying\n",
      "below\t\t\t\tbelow\n",
      "15°c\t\t\t\t15°c\n",
      "and\t\t\t\tand\n",
      "risks\t\t\t\trisk\n",
      "setting\t\t\t\tsetting\n",
      "off\t\t\t\toff\n",
      "irreversible\t\t\tirreversible\n",
      "chain\t\t\t\tchain\n",
      "reactions\t\t\treaction\n",
      "beyond\t\t\t\tbeyond\n",
      "human\t\t\t\thuman\n",
      "control\t\t\t\tcontrol\n",
      "50%\t\t\t\t50%\n",
      "may\t\t\t\tmay\n",
      "be\t\t\t\tbe\n",
      "acceptable\t\t\tacceptable\n",
      "to\t\t\t\tto\n",
      "you\t\t\t\tyou\n",
      "but\t\t\t\tbut\n",
      "those\t\t\t\tthose\n",
      "numbers\t\t\t\tnumber\n",
      "do\t\t\t\tdo\n",
      "not\t\t\t\tnot\n",
      "include\t\t\t\tinclude\n",
      "tipping\t\t\t\ttipping\n",
      "points\t\t\t\tpoint\n",
      "most\t\t\t\tmost\n",
      "feedback\t\t\tfeedback\n",
      "loops\t\t\t\tloop\n",
      "additional\t\t\tadditional\n",
      "warming\t\t\t\twarming\n",
      "hidden\t\t\t\thidden\n",
      "by\t\t\t\tby\n",
      "toxic\t\t\t\ttoxic\n",
      "air\t\t\t\tair\n",
      "pollution\t\t\tpollution\n",
      "and\t\t\t\tand\n",
      "all\t\t\t\tall\n",
      "the\t\t\t\tthe\n",
      "aspects\t\t\t\taspect\n",
      "of\t\t\t\tof\n",
      "equity\t\t\t\tequity\n",
      "and\t\t\t\tand\n",
      "climate\t\t\t\tclimate\n",
      "justice\t\t\t\tjustice\n",
      "they\t\t\t\tthey\n",
      "also\t\t\t\talso\n",
      "rely\t\t\t\trely\n",
      "on\t\t\t\ton\n",
      "my\t\t\t\tmy\n",
      "generation\t\t\tgeneration\n",
      "sucking\t\t\t\tsucking\n",
      "hundreds\t\t\thundred\n",
      "of\t\t\t\tof\n",
      "billions\t\t\tbillion\n",
      "of\t\t\t\tof\n",
      "tons\t\t\t\tton\n",
      "of\t\t\t\tof\n",
      "your\t\t\t\tyour\n",
      "co2\t\t\t\tco2\n",
      "out\t\t\t\tout\n",
      "of\t\t\t\tof\n",
      "the\t\t\t\tthe\n",
      "air\t\t\t\tair\n",
      "with\t\t\t\twith\n",
      "technologies\t\t\ttechnology\n",
      "that\t\t\t\tthat\n",
      "barely\t\t\t\tbarely\n",
      "exist\t\t\t\texist\n",
      "so\t\t\t\tso\n",
      "a\t\t\t\ta\n",
      "50%\t\t\t\t50%\n",
      "risk\t\t\t\trisk\n",
      "is\t\t\t\tis\n",
      "simply\t\t\t\tsimply\n",
      "not\t\t\t\tnot\n",
      "acceptable\t\t\tacceptable\n",
      "for\t\t\t\tfor\n",
      "us\t\t\t\tu\n",
      "we\t\t\t\twe\n",
      "who\t\t\t\twho\n",
      "have\t\t\t\thave\n",
      "to\t\t\t\tto\n",
      "live\t\t\t\tlive\n",
      "with\t\t\t\twith\n",
      "the\t\t\t\tthe\n",
      "consequences\t\t\tconsequence\n",
      "to\t\t\t\tto\n",
      "have\t\t\t\thave\n",
      "a\t\t\t\ta\n",
      "67%\t\t\t\t67%\n",
      "chance\t\t\t\tchance\n",
      "of\t\t\t\tof\n",
      "staying\t\t\t\tstaying\n",
      "below\t\t\t\tbelow\n",
      "a\t\t\t\ta\n",
      "15°c\t\t\t\t15°c\n",
      "global\t\t\t\tglobal\n",
      "temperature\t\t\ttemperature\n",
      "rise\t\t\t\trise\n",
      "the\t\t\t\tthe\n",
      "best\t\t\t\tbest\n",
      "odds\t\t\t\todds\n",
      "given\t\t\t\tgiven\n",
      "by\t\t\t\tby\n",
      "the\t\t\t\tthe\n",
      "ipcc\t\t\t\tipcc\n",
      "the\t\t\t\tthe\n",
      "world\t\t\t\tworld\n",
      "had\t\t\t\thad\n",
      "420\t\t\t\t420\n",
      "gigatons\t\t\tgigatons\n",
      "of\t\t\t\tof\n",
      "co2\t\t\t\tco2\n",
      "to\t\t\t\tto\n",
      "emit\t\t\t\temit\n",
      "back\t\t\t\tback\n",
      "on\t\t\t\ton\n",
      "january\t\t\t\tjanuary\n",
      "1st\t\t\t\t1st\n",
      "2018\t\t\t\t2018\n",
      "today\t\t\t\ttoday\n",
      "that\t\t\t\tthat\n",
      "figure\t\t\t\tfigure\n",
      "has\t\t\t\tha\n",
      "already\t\t\t\talready\n",
      "down\t\t\t\tdown\n",
      "to\t\t\t\tto\n",
      "less\t\t\t\tle\n",
      "than\t\t\t\tthan\n",
      "350\t\t\t\t350\n",
      "gigatons\t\t\tgigatons\n",
      "how\t\t\t\thow\n",
      "dare\t\t\t\tdare\n",
      "you\t\t\t\tyou\n",
      "pretend\t\t\t\tpretend\n",
      "that\t\t\t\tthat\n",
      "this\t\t\t\tthis\n",
      "can\t\t\t\tcan\n",
      "be\t\t\t\tbe\n",
      "solved\t\t\t\tsolved\n",
      "with\t\t\t\twith\n",
      "business\t\t\tbusiness\n",
      "as\t\t\t\ta\n",
      "usual\t\t\t\tusual\n",
      "and\t\t\t\tand\n",
      "some\t\t\t\tsome\n",
      "technical\t\t\ttechnical\n",
      "solutions\t\t\tsolution\n",
      "with\t\t\t\twith\n",
      "today’s\t\t\t\ttoday’s\n",
      "emission\t\t\temission\n",
      "levels\t\t\t\tlevel\n",
      "that\t\t\t\tthat\n",
      "remaining\t\t\tremaining\n",
      "co2\t\t\t\tco2\n",
      "budget\t\t\t\tbudget\n",
      "will\t\t\t\twill\n",
      "be\t\t\t\tbe\n",
      "entirely\t\t\tentirely\n",
      "gone\t\t\t\tgone\n",
      "in\t\t\t\tin\n",
      "less\t\t\t\tle\n",
      "than\t\t\t\tthan\n",
      "eight-and-a0-half\t\teight-and-a0-half\n",
      "years\t\t\t\tyear\n",
      "there\t\t\t\tthere\n",
      "will\t\t\t\twill\n",
      "not\t\t\t\tnot\n",
      "be\t\t\t\tbe\n",
      "any\t\t\t\tany\n",
      "solutions\t\t\tsolution\n",
      "or\t\t\t\tor\n",
      "plans\t\t\t\tplan\n",
      "presented\t\t\tpresented\n",
      "in\t\t\t\tin\n",
      "line\t\t\t\tline\n",
      "with\t\t\t\twith\n",
      "these\t\t\t\tthese\n",
      "figures\t\t\t\tfigure\n",
      "here\t\t\t\there\n",
      "today\t\t\t\ttoday\n",
      "because\t\t\t\tbecause\n",
      "these\t\t\t\tthese\n",
      "numbers\t\t\t\tnumber\n",
      "are\t\t\t\tare\n",
      "too\t\t\t\ttoo\n",
      "uncomfortable\t\t\tuncomfortable\n",
      "and\t\t\t\tand\n",
      "you\t\t\t\tyou\n",
      "are\t\t\t\tare\n",
      "still\t\t\t\tstill\n",
      "not\t\t\t\tnot\n",
      "mature\t\t\t\tmature\n",
      "enough\t\t\t\tenough\n",
      "to\t\t\t\tto\n",
      "tell\t\t\t\ttell\n",
      "it\t\t\t\tit\n",
      "like\t\t\t\tlike\n",
      "it\t\t\t\tit\n",
      "is\t\t\t\tis\n",
      "you\t\t\t\tyou\n",
      "are\t\t\t\tare\n",
      "failing\t\t\t\tfailing\n",
      "us\t\t\t\tu\n",
      "but\t\t\t\tbut\n",
      "the\t\t\t\tthe\n",
      "young\t\t\t\tyoung\n",
      "people\t\t\t\tpeople\n",
      "are\t\t\t\tare\n",
      "starting\t\t\tstarting\n",
      "to\t\t\t\tto\n",
      "understand\t\t\tunderstand\n",
      "your\t\t\t\tyour\n",
      "betrayal\t\t\tbetrayal\n",
      "the\t\t\t\tthe\n",
      "eyes\t\t\t\teye\n",
      "of\t\t\t\tof\n",
      "all\t\t\t\tall\n",
      "future\t\t\t\tfuture\n",
      "generations\t\t\tgeneration\n",
      "are\t\t\t\tare\n",
      "upon\t\t\t\tupon\n",
      "you\t\t\t\tyou\n",
      "and\t\t\t\tand\n",
      "if\t\t\t\tif\n",
      "you\t\t\t\tyou\n",
      "choose\t\t\t\tchoose\n",
      "to\t\t\t\tto\n",
      "fail\t\t\t\tfail\n",
      "us\t\t\t\tu\n",
      "i\t\t\t\ti\n",
      "say\t\t\t\tsay\n",
      "we\t\t\t\twe\n",
      "will\t\t\t\twill\n",
      "never\t\t\t\tnever\n",
      "forgive\t\t\t\tforgive\n",
      "you\t\t\t\tyou\n",
      "we\t\t\t\twe\n",
      "will\t\t\t\twill\n",
      "not\t\t\t\tnot\n",
      "let\t\t\t\tlet\n",
      "you\t\t\t\tyou\n",
      "get\t\t\t\tget\n",
      "away\t\t\t\taway\n",
      "with\t\t\t\twith\n",
      "this\t\t\t\tthis\n",
      "right\t\t\t\tright\n",
      "here\t\t\t\there\n",
      "right\t\t\t\tright\n",
      "now\t\t\t\tnow\n",
      "is\t\t\t\tis\n",
      "where\t\t\t\twhere\n",
      "we\t\t\t\twe\n",
      "draw\t\t\t\tdraw\n",
      "the\t\t\t\tthe\n",
      "line\t\t\t\tline\n",
      "the\t\t\t\tthe\n",
      "world\t\t\t\tworld\n",
      "is\t\t\t\tis\n",
      "waking\t\t\t\twaking\n",
      "up\t\t\t\tup\n",
      "and\t\t\t\tand\n",
      "change\t\t\t\tchange\n",
      "is\t\t\t\tis\n",
      "coming\t\t\t\tcoming\n",
      "whether\t\t\t\twhether\n",
      "you\t\t\t\tyou\n",
      "like\t\t\t\tlike\n",
      "it\t\t\t\tit\n",
      "or\t\t\t\tor\n",
      "not\t\t\t\tnot\n",
      "thank\t\t\t\tthank\n",
      "you\t\t\t\tyou\n",
      "\n",
      "\n",
      "List of synonyms and antonyms for 30 words\n",
      "*************************************************\n",
      "'Word: urgency'\n",
      "\"Synonyms:['urgency', 'urgency', 'urgency', 'importunity', 'urgency', 'urging']\"\n",
      "'Antonyms:[]'\n",
      "\n",
      "\n",
      "'Word: toxic'\n",
      "\"Synonyms:['toxic']\"\n",
      "\"Antonyms:['nontoxic']\"\n",
      "\n",
      "\n",
      "'Word: technologies'\n",
      "(\"Synonyms:['technology', 'engineering', 'engineering', 'engineering_science', \"\n",
      " \"'applied_science', 'technology']\")\n",
      "'Antonyms:[]'\n",
      "\n",
      "\n",
      "'Word: sucking'\n",
      "(\"Synonyms:['sucking', 'suck', 'suction', 'suck', 'suck', 'suck', 'suck_in', \"\n",
      " \"'suck', 'fellate', 'suck', 'blow', 'go_down_on', 'absorb', 'suck', 'imbibe', \"\n",
      " \"'soak_up', 'sop_up', 'suck_up', 'draw', 'take_in', 'take_up', 'breastfeed', \"\n",
      " \"'suckle', 'suck', 'nurse', 'wet-nurse', 'lactate', 'give_suck']\")\n",
      "\"Antonyms:['bottlefeed']\"\n",
      "\n",
      "\n",
      "'Word: situation'\n",
      "(\"Synonyms:['situation', 'state_of_affairs', 'situation', 'position', \"\n",
      " \"'situation', 'site', 'situation', 'position', 'post', 'berth', 'office', \"\n",
      " \"'spot', 'billet', 'place', 'situation']\")\n",
      "'Antonyms:[]'\n",
      "\n",
      "\n",
      "'Word: setting'\n",
      "(\"Synonyms:['setting', 'scene', 'setting', 'background', 'scope', \"\n",
      " \"'mise_en_scene', 'stage_setting', 'setting', 'context', 'circumstance', \"\n",
      " \"'setting', 'setting', 'place_setting', 'setting', 'mount', 'setting', 'put', \"\n",
      " \"'set', 'place', 'pose', 'position', 'lay', 'determine', 'set', 'specify', \"\n",
      " \"'set', 'determine', 'define', 'fix', 'limit', 'set', 'mark', 'set', 'set', \"\n",
      " \"'fix', 'prepare', 'set_up', 'ready', 'gear_up', 'set', 'set', 'set', \"\n",
      " \"'localize', 'localise', 'place', 'set', 'go_down', 'go_under', 'arrange', \"\n",
      " \"'set', 'plant', 'set', 'set', 'jell', 'set', 'congeal', 'typeset', 'set', \"\n",
      " \"'set', 'set', 'countersink', 'set', 'sic', 'set', 'place', 'put', 'set', \"\n",
      " \"'rig', 'set', 'set_up', 'set_up', 'lay_out', 'set', 'adjust', 'set', \"\n",
      " \"'correct', 'fructify', 'set', 'dress', 'arrange', 'set', 'do', 'coif', \"\n",
      " \"'coiffe', 'coiffure']\")\n",
      "\"Antonyms:['rise']\"\n",
      "\n",
      "\n",
      "'Word: presented'\n",
      "(\"Synonyms:['show', 'demo', 'exhibit', 'present', 'demonstrate', 'present', \"\n",
      " \"'represent', 'lay_out', 'stage', 'present', 'represent', 'present', \"\n",
      " \"'submit', 'present', 'pose', 'award', 'present', 'give', 'gift', 'present', \"\n",
      " \"'deliver', 'present', 'introduce', 'present', 'acquaint', 'portray', \"\n",
      " \"'present', 'confront', 'face', 'present', 'present', 'salute', 'present']\")\n",
      "'Antonyms:[]'\n",
      "\n",
      "\n",
      "'Word: plea'\n",
      "\"Synonyms:['supplication', 'plea', 'plea', 'plea']\"\n",
      "'Antonyms:[]'\n",
      "\n",
      "\n",
      "'Word: nowhere'\n",
      "\"Synonyms:['nowhere', 'nowhere']\"\n",
      "'Antonyms:[]'\n",
      "\n",
      "\n",
      "'Word: odds'\n",
      "\"Synonyms:['odds', 'odds', 'betting_odds']\"\n",
      "'Antonyms:[]'\n",
      "\n",
      "\n",
      "'Word: matter'\n",
      "(\"Synonyms:['matter', 'affair', 'thing', 'topic', 'subject', 'issue', \"\n",
      " \"'matter', 'matter', 'matter', 'matter', 'matter', 'count', 'matter', \"\n",
      " \"'weigh']\""
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "class nltkSpeech():\n",
    "    \n",
    "    def __init__(self, url=\"https://www.nme.com/features/greta-thunberg-full-speech-to-the-un-2019-climate-change-summit-2550824\"):\n",
    "        self.speech = []\n",
    "        self.url = url\n",
    "        \n",
    "\n",
    "    def getSpeech(self):\n",
    "        res = requests.get(self.url)\n",
    "        htmlPage = res.content\n",
    "        soup = BeautifulSoup(htmlPage, 'lxml')\n",
    "        text= []\n",
    "        dis = []\n",
    "        firstp = soup.p    \n",
    "        for x in firstp.find_all_next('p'):\n",
    "            sentence = x.text.strip()\n",
    "           # set_trace()\n",
    "            if not (sentence.startswith(\"Everyone\") or sentence.startswith(\"NME\") or sentence.startswith('©')):   \n",
    "                self.speech.extend(re.sub(r'[.,()“”!?]','',sentence).lower().split())  \n",
    "        \n",
    "    \n",
    "    def freSort(self):\n",
    "        print(\"List of words sorted by frequency\")\n",
    "        print(\"*************************************************\")\n",
    "        counts = Counter(self.speech)\n",
    "        pp.pprint(counts)\n",
    "        print('\\n')\n",
    "    \n",
    "    def alphaSort(self):\n",
    "        print(\"List of words sorted alphabetically\")#Q: should duplidate words be listed just once?\n",
    "        print(\"*************************************************\")\n",
    "        pp.pprint(sorted(self.speech))\n",
    "        print('\\n')\n",
    "        \n",
    "   \n",
    "    \n",
    "    def lemmatizing(self):\n",
    "        print('Lemmatizing all words with WordNetLemmatizer\\n')\n",
    "        print(\"*************************************************\")\n",
    "        print('Word\\t\\t\\t\\tLemma')\n",
    "        print('-------------------------------------------------')\n",
    "        lm = nltk.stem.WordNetLemmatizer()\n",
    "        \n",
    "        for e in self.speech:\n",
    "            lol=int(len(e)/8)\n",
    "            if lol == 1:\n",
    "                print(e+'\\t\\t\\t'+lm.lemmatize(e))\n",
    "            elif lol == 2:\n",
    "                print(e+'\\t\\t'+lm.lemmatize(e))\n",
    "            else:\n",
    "                print(e+'\\t\\t\\t\\t'+lm.lemmatize(e))         \n",
    "            \n",
    "        print('\\n')    \n",
    "            \n",
    "    def syn(self):\n",
    "        words = ['urgency','toxic','technologies','sucking','situation','setting','presented','plea','nowhere','odds','matter','leaders','justice','kept','irreversible','impassioned','heartfelt','gigatons','extinction','equity','eternal','crystal','consequences','childhood','betrayal','billions','aspects','additional','activist']\n",
    "        print(\"List of synonyms and antonyms for 30 words\")\n",
    "        print(\"*************************************************\")\n",
    "        for word in words:\n",
    "            synonyms = []\n",
    "            antonyms = []\n",
    "            for sqyn in wordnet.synsets(word):\n",
    "                for lm in sqyn.lemmas():\n",
    "                    synonyms.append(lm.name())\n",
    "                    if lm.antonyms():\n",
    "                        antonyms.append(lm.antonyms()[0].name())\n",
    "            pp.pprint('Word: ' + str(word))\n",
    "            pp.pprint('Synonyms:' + str(synonyms))\n",
    "            pp.pprint('Antonyms:' + str(antonyms))\n",
    "            \n",
    "            print('\\n')\n",
    "    \n",
    "    def run(self):\n",
    "        self.getSpeech()\n",
    "        self.freSort()\n",
    "        self.alphaSort()\n",
    "        self.lemmatizing()\n",
    "        self.syn()\n",
    "\n",
    "ntlkspeech=nltkSpeech()\n",
    "ntlkspeech.run()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T21:33:45.506875Z",
     "start_time": "2020-01-19T21:33:45.479872Z"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown(\n",
    "\"\"\"\n",
    "# Python Exam (For Python students):\n",
    "<ol>\n",
    "<li>(100 points)Write a program that contains a while loop that repeatedly asks the user to enter numeric miles they run each week.  Append each run to a list called “distance”. You may use this format [(“week1”, 23.1), (“week2”, 12.2)…]  \n",
    "</li><br><li>(100 points)Write a program that get the max, min, average, number of runs etc. \n",
    "</li><br><li>(100 points)Write a program that prompts the user to enter in an unlimited number of colors. Test to make sure that the user did not already enter a color before storing it (i.e. “We’re sorry, but you’ve already added the color ‘yellow’”). When the user types the string “exit” you should stop collecting colors. The print out the colors entered in alphabetical order.  \n",
    "</li><br><li>(100 points)Write a function that generates a random letter from the string ‘BINGO’. Here is a sample running of this program: x = generate_bingo_letter() print (x) \n",
    "</li><br><li>(100 points)Define a function fraction(fstring), where the argument passed in stores a fraction as a string. The function should return a floating point number that is the value of the fraction. \n",
    " \n",
    "Examples of using the function: <br>\n",
    "         print(fraction(“3/4”))  # prints  .750 <br>\n",
    "         print(fraction(“1/3”))  # prints   0.333 <br>\n",
    "     \n",
    " \n",
    "</li><br><li>(Bonus. 300 Points) Write a function balance_parens(astring) that inputs a string with parenthesized sub-­‐strings, which may be nested (e.g., could be a Python arithmetic expression). The function returns True if all of the parentheses that are opened are properly closed, otherwise it returns False. \n",
    "<br>\n",
    "For example, the following string is balanced: \n",
    "<br>\n",
    "“ A * ( (B + C) – (D – E) )” \n",
    "<br>\n",
    "And the following string is not balanced: \n",
    "<br> \n",
    "“  A ((B * C)  + (D      \n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T21:14:11.821733Z",
     "start_time": "2020-01-19T21:14:11.814757Z"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown(\n",
    "\"\"\"\n",
    "## You may use the following libraries for the exams. \n",
    "import re <br>\n",
    "import spacy  <br>\n",
    "from spacy import displacy <br>\n",
    "import en_core_web_sm <br>\n",
    "from pathlib import Path <br>\n",
    "from IPython.display import display, Markdown, Latex  <br>\n",
    "import sys <br>\n",
    "import csv <br>\n",
    "import pandas as pd <br>\n",
    "import numpy as np <br>\n",
    "import os <br>\n",
    "from os import path <br>\n",
    "from datetime import datetime <br>\n",
    "from urllib.parse import urlparse, urljoin <br>\n",
    "import requests <br>\n",
    "import ntpath <br>\n",
    "from bs4 import BeautifulSoup, Comment <br>\n",
    "import time <br>\n",
    "from pprint import pprint  <br>\n",
    "import shutil <br>\n",
    "import tldextract <br>\n",
    "import multiprocessing <br>\n",
    "from multiprocessing import Process, Queue, cpu_count <br>\n",
    "import os <br>\n",
    "from os import path, listdir  <br>\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "def run(weeks=10):\n",
    "    distance = []\n",
    "    week = 1\n",
    "    while week <= weeks:\n",
    "        try:\n",
    "            miles = input('How many miles have you run in week' + str(week) + '\\n')\n",
    "            milesint = float(miles)\n",
    "            pair = ('week' + str(week), milesint)\n",
    "            week += 1\n",
    "            distance.append(pair)\n",
    "        except ValueError:\n",
    "            print(\"Please enter a number\")\n",
    "            continue\n",
    "\n",
    "            \n",
    "    return(distance)\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "y=0\n",
    "data= [('week1', 23.0),\n",
    " ('week2', 4334.0),\n",
    " ('week3', 324.0),\n",
    " ('week4', 4.0),\n",
    " ('week5', 35321.0),\n",
    " ('week6', 562134.0),\n",
    " ('week7', 43214.0),\n",
    " ('week8', 5.0),\n",
    " ('week9', 2354.0),\n",
    " ('week10', 5.0)]\n",
    "\n",
    "number_of_runs = len(data)\n",
    "\n",
    "max_miles=max(data, key=lambda x: x[1])\n",
    "min_miles=min(data, key=lambda x: x[1])\n",
    "avg=sum(x[1] for x in data)/number_of_runs\n",
    "\n",
    "print(\"The farthest distance you've run is \" + str(max_miles[1]) + ' miles ' + 'on ' + str(max_miles[0]))\n",
    "print(\"The shortest distance you've run is \" + str(min_miles[1]) + ' miles ' + 'on ' + str(min_miles[0]))\n",
    "print(\"Your average distance ran is \" + str(avg) + \" miles for \" + str(number_of_runs))\n",
    "print(\"You've gone on \" + str(number_of_runs) + \" runs so far.\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "def listcolors():\n",
    "    colors = []\n",
    "    while True:\n",
    "        usercolor = input(\"Add a color: \")\n",
    "        if usercolor == \"exit\":\n",
    "            break\n",
    "            \n",
    "        if usercolor in colors:\n",
    "            print(\"We’re sorry, but you’ve already added the color \" + usercolor)\n",
    "            continue\n",
    "        else:\n",
    "            colors.append(usercolor)\n",
    "        \n",
    "    print(sorted(colors))\n",
    "            \n",
    "listcolors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def generate_bingo_letter():\n",
    "    bingo=\"BINGO\"\n",
    "    \n",
    "    rand = random.randint(0,4)        \n",
    "    return bingo[rand]\n",
    "    \n",
    "    #return random.choice(bingo)\n",
    "\n",
    "\n",
    "x = generate_bingo_letter()\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "def fraction(fstring):\n",
    "    sum = 0\n",
    "    splitfract=fstring.split()\n",
    "    if len(splitfract) != 1 and len(splitfract) != 2:\n",
    "        print(\"invalid fraction\")\n",
    "        sys.exit()\n",
    "        \n",
    "       \n",
    "    for x in splitfract:\n",
    "        try:\n",
    "            a = Fraction(x)\n",
    "            rounded = round(float(a),3)\n",
    "            sum = sum + rounded\n",
    "        except ValueError as e :\n",
    "            print(e)\n",
    "    return(sum)\n",
    "\n",
    "\n",
    "fraction('74398231 3841234/1324132')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "def balance_parens(astring):\n",
    "    thestack = []\n",
    "    index = 0\n",
    "    \n",
    "    \n",
    "    while index < len(astring):\n",
    "        lpar = astring[index]\n",
    "        if lpar == \"(\":\n",
    "            thestack.append(lpar)\n",
    "        elif lpar == ')':\n",
    "            if len(thestack) == 0:\n",
    "                return False\n",
    "            else:\n",
    "                thestack.pop()\n",
    "        index = index + 1\n",
    "        \n",
    "        \n",
    "    if len(thestack) == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "balance_parens(\"(((((kdkdksadjiajsd))))\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
