{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 bold=true>Dec 15 </font>\n",
    "## Parse One Page. And Logics to parse a full website\n",
    "## Make sure of libraries\n",
    "## Review Python variables and data structures\n",
    "## Brainstorm: define a data structure to store (implementation next lesson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T01:40:57.926586Z",
     "start_time": "2019-12-15T01:40:57.921587Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from urllib.error import HTTPError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T06:55:27.675674Z",
     "start_time": "2019-12-15T06:55:27.659644Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-3042b29308ad>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-3042b29308ad>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    length = base_url.endWith('/')? len(base_url)-1: len(base_url)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "need_tag_list=['a', \n",
    "     'body',   \n",
    "     'div',\n",
    "     'h1',\n",
    "     'h2',\n",
    "     'h3',\n",
    "     'h4',\n",
    "     'h5',\n",
    "     'head',\n",
    "     'header',\n",
    "     'label',\n",
    "     'li',\n",
    "     'meta',\n",
    "     'p',\n",
    "     'script',\n",
    "     'span',\n",
    "     'strong',\n",
    "     'textarea',\n",
    "     'title',\n",
    "     'ul']\n",
    "#s=set([re.sub(r'[_+!@#$?\\\\/^]+$', '', item) for item in strings])\n",
    "base_url=\"https://www.washington.edu/\"\n",
    "base_domain='www.washington.edu'\n",
    "base_url=re.sub(r'[_+!@#$?\\\\\\s]+$', '', base_url)\n",
    "base_url=re.sub(r'[/\\s]$','', base_url) \n",
    "\n",
    "def all_page(base_url, links_only=True):\n",
    "    firstIndex = base_url.find(\"//\");\n",
    "    length = base_url.endWith('/')? len(base_url)-1: len(base_url)\n",
    "    base_domain=base_url[index+1:length]\n",
    "    base_url=re.sub(r'[_+!@#$?\\\\\\s]+$', '', base_url)\n",
    "    base_url=re.sub(r'[/\\s]$','', base_url) \n",
    "    response=requests.get(base_url)\n",
    "    unique_urls={base_url}\n",
    "    visited_urls=set()\n",
    "    unables=set()\n",
    "    while len(unique_urls)>len(visited_urls):\n",
    "        soup=BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            try:\n",
    "                url=link['href']                \n",
    "                parsed_uri = urlparse(url)\n",
    "                if parsed_uri.netloc=='':\n",
    "                    absolute_url=base_url+url    \n",
    "                elif parsed_uri.netloc==base_domain:\n",
    "                    absolute_url=url\n",
    "                else:\n",
    "                    continue\n",
    "                clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url)\n",
    "                clean=''.join(clean[0:10])+(''.join(clean[11:])).replace('//', '/')\n",
    "                clean=re.sub(r'[/\\s]$','', clean)\n",
    "                unique_urls.add(clean)                  \n",
    "            except:\n",
    "                continue            \n",
    "        unvisited_url=(unique_urls-visited_urls-unables).pop()\n",
    "        visited_urls.add(unvisited_url)\n",
    "        response=requests.get(unvisited_url)\n",
    "        if links_only!=True and response.status_code=='200': #use a different routine to get text data from resonse. \n",
    "            handleOnePage(unvisited_url)\n",
    "        else:\n",
    "            unables.add(unvisited_url)  \n",
    "    return unique_urls\n",
    "  \n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pages(self, base_url, links_only=True):\n",
    "            response=requests.get(base_url)\n",
    "            allUrls={base_url}\n",
    "            visitedUrls=set()\n",
    "            rejectedUrls=set()\n",
    "            while len(allUrls)>len(visitedUrls):\n",
    "                soup=BeautifulSoup(response.text, 'html.parser')\n",
    "                for link in soup.find_all('a'):\n",
    "                    try:\n",
    "                        url=link['href']                \n",
    "                        parsed_uri = urlparse(url )\n",
    "                        if parsed_uri.netloc=='':\n",
    "                            absolute_url=base_url+url    \n",
    "                        elif parsed_uri.netloc==base_domain:\n",
    "                            absolute_url=url\n",
    "                        else:\n",
    "                            continue\n",
    "                        clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url).replace('.edu','.edu/').replace('.edu//','.edu/')\n",
    "                        allUrls.add(clean)               \n",
    "                    except:\n",
    "                        continue\n",
    "                unvisited_url=(allUrls-visitedUrls-rejectedUrls).pop()\n",
    "                visitedUrls.add(unvisited_url)\n",
    "                response=requests.get(unvisited_url)\n",
    "                if response.status_code!=200:\n",
    "                    rejectedUrls.add(unvisited_url)  \n",
    "            return [allUrls[0:max_Pages],visitedUrls,rejectedUrls] if len(allUrls)>150 else [allUrls,visitedUrls,rejectedUrls]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all_pages() missing 1 required positional argument: 'base_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7e8c263aa9d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"http://www.ucla.edu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: all_pages() missing 1 required positional argument: 'base_url'"
     ]
    }
   ],
   "source": [
    "all_pages(\"http://www.ucla.edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-94852a7321c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"http://www.ucla.edu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_page' is not defined"
     ]
    }
   ],
   "source": [
    "print(all_page(\"http://www.ucla.edu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T16:07:13.367017Z",
     "start_time": "2019-12-15T16:07:13.360057Z"
    }
   },
   "outputs": [],
   "source": [
    "def handleOnePage(unvisited_url):\n",
    "    response = requests.get(unvisited_url) \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    in_the_page=set([tag.name for tag in soup.find_all()]) \n",
    " \n",
    "    go_throug=in_the_page.intersection(need_tag_list) \n",
    "    \n",
    "    for tag in go_throug:   \n",
    "        sub_soups=soup.find_all(tag) \n",
    "        for one_by_one in sub_soups: \n",
    "            data1=one_by_one.findAll(text=True)\n",
    "            r1=filter(visible, data1)\n",
    "            textcontent=[i for i in r1 if len(i)>2]   \n",
    "            print(textcontent)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T16:07:41.413722Z",
     "start_time": "2019-12-15T16:07:40.436008Z"
    }
   },
   "outputs": [],
   "source": [
    "#choose a college of your choice\n",
    "while True:\n",
    "    try:\n",
    "        test = input(\"Which website would you like to go today?\")\n",
    "        handleOnePage(test)\n",
    "        break\n",
    "    except HTTPError as err:\n",
    "        print(err)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-14T21:03:49.481803Z",
     "start_time": "2019-12-14T21:03:49.475801Z"
    }
   },
   "outputs": [],
   "source": [
    "go_throug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for onedevi in soup.find_all('div', class_='slide-menu__level-1__item d-flex'):\n",
    "   text.append(handleOnePage(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='Colleges.txt'\n",
    "df_text=pd.DataFrame(text, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages(\"http://www.washington.edu/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
