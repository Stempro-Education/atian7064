{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 bold=true>Feb 16, 2020  Part A\n",
    "<br><br>Content cleaning</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T06:44:30.916722Z",
     "start_time": "2020-02-16T06:44:30.908729Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, Latex \n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path, listdir\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import requests\n",
    "import ntpath\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "from pprint import pprint \n",
    "import shutil\n",
    "#!pip install tldextract\n",
    "import tldextract\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:42:35.760091Z",
     "start_time": "2020-02-16T18:42:35.726124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nab=Get_Url_content()\\nu,d=ab.Merge_url_and_data() \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Get_Url_content():\n",
    "    def __init__(self, \n",
    "                 folder='C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2020feb16\\\\learning_data', \n",
    "                 result_folder='C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2020feb16\\\\learning_data\\\\result',\n",
    "                 data_pattern='^A_.+\\d+.csv$', \n",
    "                 url_file_pattern='A_.+\\d+url.csv$',\n",
    "                 college_list_file='C:\\\\Users\\\\Kentt\\\\stempro\\\\hw\\\\y2020feb16\\\\learning_data\\\\ContentAqusition__All.csv', \n",
    "                 data_sum_file='learning_data_sum.csv', \n",
    "                 url_sum_file='learning_url_sum.csv',\n",
    "                 data_info_file='learning_data_info.csv'\n",
    "                ):\n",
    "        self.folder=folder\n",
    "        self.result_folder=result_folder\n",
    "        self.data_pattern=data_pattern\n",
    "        self.url_file_pattern=url_file_pattern\n",
    "        self.college_list_file=college_list_file\n",
    "        self.data_sum_file=data_sum_file\n",
    "        self.url_sum_file=url_sum_file\n",
    "        self.data_info_file=data_info_file\n",
    "    def Get_Files(self, pattern):\n",
    "        return [[os.stat(path.join(self.folder, f)).st_size, path.join(self.folder, f), f] \n",
    "                for f in listdir(self.folder) if re.match(pattern, f)]\n",
    "    def Get_data_info(self): \n",
    "        #data files\n",
    "        #curl=Get_Url_content('C:/Stempro/Notebooks/data')\n",
    "        data_files=self.Get_Files(self.data_pattern)\n",
    "        data_files.sort(key = lambda x: x[0],reverse=True) \n",
    "        #to df\n",
    "        df_files=pd.DataFrame({'size_bytes':[c1[0]/1000.0 for c1 in data_files], 'full_path':[c1[1] for c1 in data_files], 'basename':[c1[2] for c1 in data_files]})\n",
    "        df_files['accumu_size']=df_files['size_bytes'].apply('cumsum')\n",
    "        df_files['per']=100.0*df_files['accumu_size']/df_files['size_bytes'].apply('sum')\n",
    "        \n",
    "        #url files \n",
    "        url_files=self.Get_Files(self.url_file_pattern)\n",
    "        df_url_files=pd.DataFrame({'size_bytes':[c1[0] for c1 in url_files], 'full_path':[c1[1] for c1 in url_files], 'basename':[c1[2] for c1 in url_files]})\n",
    "        \n",
    "        merge_file_stats=pd.concat([pd.read_csv(i[1][1], sep='\\t').groupby('status_code')['url'].count().reset_index().assign(college=i[1][2]) for \n",
    "        i in df_url_files.iterrows()], axis=0)  \n",
    "        \n",
    "        df_stats=merge_file_stats.groupby('college').apply(self.Agg_college).reset_index()\n",
    "         \n",
    "        #master file df \n",
    "        df_master=pd.read_csv(self.college_list_file, sep=',') \n",
    "        \n",
    "        df_master=df_master[['name', 'url', 'filename', 'urlfile']].merge(df_stats[['college', 'total_pages','visited', 'rejected']], \n",
    "            left_on='urlfile', right_on='college', how='left')\n",
    "        df_master=df_master.merge(df_files[['basename', 'size_bytes', 'accumu_size', 'per']], left_on='filename', \n",
    "                              right_on='basename', how='left')\n",
    "        df_master.fillna('', inplace=True) \n",
    "        df_master.to_csv(path.join(self.result_folder, self.data_info_file), sep='\\t', index=False) \n",
    "        return df_master.head(100)\n",
    "    \n",
    "    def Assign_url_id(self):\n",
    "        datafiles=self.Get_Files(self.data_pattern)\n",
    "        huge=pd.concat([pd.read_csv(i[1], sep='\\t')[['url', 'text']].groupby('url')['text'].count().reset_index().assign(college=i[2]) \n",
    "                        for i in datafiles], axis=0)  \n",
    "        huge.reset_index(inplace=True)\n",
    "        df_master=pd.read_csv(self.college_list_file, sep=',') \n",
    "        #df_master=pd.read_csv( 'C:/Stempro/Notebooks/training/ContentAqusition__All.csv', sep=',') \n",
    "        huge=huge.merge(df_master[['collegeId', 'url', 'name', 'filename']], \n",
    "                        left_on='college', right_on='filename', how='right')\n",
    "\n",
    "        huge.rename(columns={'index': 'url_id', 'text': 'lines', 'url_x':'url', 'url_y': 'college_url', 'college':'data_file'}, inplace=True) \n",
    "        huge.drop(columns=['filename'], inplace=True) \n",
    "        return huge \n",
    "    \n",
    "    def Clean_one_file(self, file):\n",
    "        return pd.read_csv(self.college_list_file, sep=',')\n",
    "    \n",
    "    def Agg_college(self,x):\n",
    "        d = {}\n",
    "        d['total_pages'] = x['url'].sum() \n",
    "        d['visited'] = x[x['status_code']==200]['url'].sum() \n",
    "        d['rejected'] = x[(x['status_code']!=200) & (x['status_code']!=0)]['url'].sum() \n",
    "        return pd.Series(d, index=['total_pages', 'visited', 'rejected'])\n",
    "    def Agg_url(self,x): \n",
    "        d = {}\n",
    "        d['entries'] = x['text'].count() \n",
    "        d['textlen'] = x['len'].sum()  \n",
    "        return pd.Series(d, index=['entries', 'textlen'])\n",
    "    \n",
    "    def Merge_url_and_data(self):\n",
    "        datafiles=self.Get_Files(self.data_pattern)\n",
    "        df_url_sum=pd.DataFrame()\n",
    "        df_data_sum=pd.DataFrame() \n",
    "        for i in datafiles:\n",
    "            try:\n",
    "                file=i[1]\n",
    "                college_id=int(''.join(j for j in i[2] if j.isdigit()))     \n",
    "                dfdata=pd.read_csv(file, sep='\\t') \n",
    "                dfdata.fillna('', inplace=True) \n",
    "                dfdata['text']=dfdata['text'].astype(str)\n",
    "                dfdata['len']=dfdata['text'].apply((lambda x: len(x)))\n",
    "                dfdata=dfdata[~dfdata['url'].str.contains('.pdf', case=False)]\n",
    "                \n",
    "                df_urls=dfdata.groupby('url').apply(self.Agg_url).reset_index().reset_index() \n",
    "                df_urls.rename(columns={'index':'id_url'}, inplace=True)\n",
    "                df_urls=df_urls.assign(id_college=college_id)\n",
    "                df_url_sum=pd.concat([df_url_sum, df_urls])\n",
    "\n",
    "                df_data_new=dfdata[['url', 'parent', 'ps', 'ns', 'text']].merge(df_urls, left_on='url', right_on='url', how='inner')\n",
    "                df_data_new.drop(columns=['url', 'textlen', 'entries'], inplace=True) \n",
    "\n",
    "                df_data_sum=pd.concat([df_data_sum, df_data_new])\n",
    "            except Exception as e:\n",
    "                print(e, file)\n",
    "        df_url_sum.to_csv(path.join(self.result_folder, self.url_sum_file), sep='\\t', index=False)\n",
    "        df_data_sum.to_csv(path.join(self.result_folder, self.data_sum_file), sep='\\t', index=False) \n",
    "        return df_url_sum.head(100), df_data_sum.head(100)\n",
    "\n",
    "\"\"\" \n",
    "ab=Get_Url_content()\n",
    "u,d=ab.Merge_url_and_data() \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T18:42:54.690532Z",
     "start_time": "2020-02-16T18:42:54.687528Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-16T18:43:00.484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    id_url  \\\n",
       " 0   0        \n",
       " 1   1        \n",
       " 2   2        \n",
       " 3   3        \n",
       " 4   4        \n",
       " .. ..        \n",
       " 95  95       \n",
       " 96  96       \n",
       " 97  97       \n",
       " 98  98       \n",
       " 99  99       \n",
       " \n",
       "                                                                        url  \\\n",
       " 0   http://admissionblog.agnesscott.org/category/decisions                   \n",
       " 1   http://admissionblog.agnesscott.org/category/journeys                    \n",
       " 2   http://agnesscott.edu/admission/high-school-students/how-to-apply.html   \n",
       " 3   http://agnesscott.edu/admission/index.html                               \n",
       " 4   http://agnesscott.edu/visit                                              \n",
       " ..                          ...                                              \n",
       " 95  http://jobs.atsu.edu                                                     \n",
       " 96  http://jobs.atsu.edu/about-atsu                                          \n",
       " 97  http://jobs.atsu.edu/afainstitute                                        \n",
       " 98  http://jobs.atsu.edu/ahec                                                \n",
       " 99  http://jobs.atsu.edu/asdoh                                               \n",
       " \n",
       "     entries  textlen  id_college  \n",
       " 0   56       1351     100000      \n",
       " 1   60       1612     100000      \n",
       " 2   207      5198     100000      \n",
       " 3   283      8918     100000      \n",
       " 4   212      7908     100000      \n",
       " ..  ...       ...        ...      \n",
       " 95  1079     24040    100000      \n",
       " 96  1079     24043    100000      \n",
       " 97  1081     24097    100000      \n",
       " 98  1079     24040    100000      \n",
       " 99  1079     24040    100000      \n",
       " \n",
       " [100 rows x 5 columns],\n",
       "    parent ps    ns                                                text  \\\n",
       " 0   a               Skip to content                                      \n",
       " 1   div       span                                           Mo.         \n",
       " 2   div       span  ° /                                                  \n",
       " 3   div       span                                              Ariz.    \n",
       " 4   div       span                                           Calif.      \n",
       " ..  ...   ..   ...                                               ...     \n",
       " 95  a               Who's Who                                            \n",
       " 96  h2              Visit ATSU                                           \n",
       " 97  p         br    Schedule your tour today or                          \n",
       " 98  p               view our virtual tours.                              \n",
       " 99  a               Learn more                                           \n",
       " \n",
       "     id_url  id_college  \n",
       " 0   108     100000      \n",
       " 1   108     100000      \n",
       " 2   108     100000      \n",
       " 3   108     100000      \n",
       " 4   108     100000      \n",
       " ..  ...        ...      \n",
       " 95  108     100000      \n",
       " 96  108     100000      \n",
       " 97  108     100000      \n",
       " 98  108     100000      \n",
       " 99  108     100000      \n",
       " \n",
       " [100 rows x 6 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab=Get_Url_content()\n",
    "ab.Get_data_info()\n",
    "ab.Merge_url_and_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
