{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 bold=true>Dec 29 -- Part B\n",
    "<br><br>Content Acquisition</font>\n",
    "\n",
    "<font size=5 color='Olive'>For the first 5 lessons, DS will be combined with Python due to insufficiencies of Concepts</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T07:05:32.675684Z",
     "start_time": "2019-12-29T07:05:32.670686Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import requests\n",
    "import ntpath\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Object Oriented Programming: Define Classes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T06:57:10.646974Z",
     "start_time": "2019-12-29T06:57:10.640004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "College: Stanford avg gpa requirement is 3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Describe_College(object):    \n",
    "    def __init__(self,col_name, avg_gpa, avg_act, avg_sat, addr): \n",
    "        self.col_name=col_name\n",
    "        self.avg_gpa=avg_gpa\n",
    "        self.avg_act=avg_act\n",
    "        self.avg_sat=avg_sat\n",
    "        self.addr=addr \n",
    "    def basic_info(self):\n",
    "        print(\"College: {} avg gpa requirement is {}\".format(self.col_name,self.avg_gpa))\n",
    "\n",
    "    def get_avggpa(self):\n",
    "        return self.avg_gpa\n",
    "\n",
    "c=Describe_College('Stanford', 3.0, 33.0, 1500, 'CA')\n",
    "c.basic_info()\n",
    "c.get_avggpa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T16:14:28.530135Z",
     "start_time": "2019-12-29T16:14:28.493129Z"
    },
    "code_folding": [
     59,
     177
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    c=CollegeCrawl('University of Washington', 'https://www.washington.edu')\n",
    "    c.GetAllUrls()\n",
    "'''\n",
    "class CollegeCrawl():\n",
    "    Gap_Insecond=5\n",
    "    Max_Pages=15      \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: https://www.university.edu. Scheme and netloc need to be complete\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then everth page \n",
    "        respectrobottxt: True\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, \n",
    "                 _prioritykeywords=['apply', 'admission', 'application', 'deadline'], \n",
    "                 _url_file=None,                  \n",
    "                 _save_to_folder=None,\n",
    "                 _existingurlfile=None, #csv files that were visited in the past\n",
    "                 _respectrobottxt=True, \n",
    "                _headers={'User-Agent':'Mozilla/5.0'} ):\n",
    "        self.college=_collegename\n",
    "         \n",
    "        if urlparse(_rooturl).scheme==\"\":\n",
    "            print('URL needs to have scheme. Please try again.')\n",
    "            raise Exception('URL needs to have scheme')            \n",
    "            return\n",
    "        \n",
    "        self.rootUrl=_rooturl\n",
    "        self.base_domain=urlparse(self.rootUrl).netloc\n",
    "        self.scheme=urlparse(self.rootUrl).scheme\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "        if _save_to_folder==None or path.isdir(_save_to_folder)==False:\n",
    "            self.save_to_folder=os.getcwd()\n",
    "        else:\n",
    "            self.save_to_folder=_save_to_folder\n",
    "            \n",
    "        #to make it less \n",
    "        if _existingurlfile==None or path.exists(_existingurlfile)==False:            \n",
    "            self.existingurlfile=path.join(self.save_to_folder,re.sub(\"\\s+\", \"_\", self.college.strip()+'.csv'))\n",
    "        else:\n",
    "            self.existingurlfile=_existingurlfile\n",
    "        self.allurls={}\n",
    "        self.headers=_headers\n",
    "        self.files=[]\n",
    "            \n",
    "    '''simple description'''        \n",
    "    def __str__(self):\n",
    "        return '{}. Starting URL: {}'.format(self.college, self.rootUrl)\n",
    "    '''\n",
    "        load _existingurlfile: two columns -- Url and status_code\n",
    "        minimum assumptions: first two columns are url and status_code\n",
    "    '''\n",
    "    def Load_DiscoveredUrls(self, delimiter=',', hasHeader=False, header_names=['url', 'status_code']):   \n",
    "        if self.existingurlfile==None:\n",
    "            return {}\n",
    "        else:\n",
    "            if path.exists(self.existingurlfile) and re.sub('\\s+', '_', self.college) in ntpath.basename(self.existingurlfile):\n",
    "                df_urls=pd.read_csv(self.existingurlfile,  delimiter=delimiter).iloc[:, 0:2]   \n",
    "                header_row=list(df_urls.columns) \n",
    "            \n",
    "                if re.match('^http', header_row[0]):\n",
    "                    df_urls.columns=header_names\n",
    "                    df_urls=pd.concat([df_urls, pd.DataFrame([header_row], columns=header_names)])                \n",
    "                else: #event it is not, still assign the header_name\n",
    "                    df_urls.columns=header_names \n",
    "                \n",
    "                return dict(zip(df_urls['url'], df_urls['status_code'])) #format: url:status_code. i.e., url is the key\n",
    "                #another options is: df_urls.to_dict('list') #format of {url:[url1, url2...], status_code:[0, 200...]}\n",
    "            else:\n",
    "                return {}        \n",
    "            \n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl \n",
    "        headers={'User-Agent':'Mozilla/5.0'}\n",
    "        #Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\n",
    "        #full list: https://www.whatismybrowser.com/guides/the-latest-user-agent/chrome\n",
    "        response= requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text)  \n",
    "        \n",
    "        it will first load which ever has alread visited \n",
    "\n",
    "    \"\"\"    \n",
    "    def GetAllUrls(self, headers=None, links_only=True): \n",
    "        #load existing urls if any\n",
    "        urls=self.Load_DiscoveredUrls() \n",
    "        \n",
    "        if len(urls)==0:\n",
    "            urls={self.rootUrl:0}  \n",
    "            unvisited=[self.rootUrl]\n",
    "        else:\n",
    "            unvisited=[url for url, status_code in urls.items() if status_code==0]\n",
    "            if not unvisited:\n",
    "                unvisited=[self.rootUrl]\n",
    "            \n",
    "        if headers==None: \n",
    "            if self.headers:\n",
    "                headers=self.headers\n",
    "            else: \n",
    "                 headers={'User-Agent':'Mozilla/5.0'}  \n",
    "                    \n",
    "        #get base_domain    \n",
    "        if self.base_domain==None:\n",
    "            self.base_domain=urlparse(unvisited[0]).netloc\n",
    "        if self.scheme==None:\n",
    "            self.scheme=urlparse(unvisited[0]).scheme\n",
    "     \n",
    "        pages_visited =0\n",
    "        try:  \n",
    "            while unvisited:        \n",
    "                pages_visited+=1\n",
    "                url=unvisited.pop()  \n",
    "                response=requests.get(url, headers=headers)     \n",
    "                status_code=response.status_code\n",
    "                urls[url]=status_code\n",
    "                 \n",
    "                if status_code==200:\n",
    "                    soup=BeautifulSoup(response.text, 'html.parser') \n",
    "                    for link in soup.find_all('a'): \n",
    "                        if link.has_attr('href'):\n",
    "                            url=link['href']       \n",
    "                            url=re.sub(r'[\\/_+!@#$?\\\\\\s]+$', '', url)\n",
    "                            parsed_uri = urlparse(url) \n",
    "                            absolute_url=''\n",
    "           \n",
    "                            if (parsed_uri.netloc=='') and (parsed_uri.scheme=='') and re.match(r'^\\/.*\\w$', parsed_uri.path) :\n",
    "                                absolute_url=urljoin(self.scheme+'://'+self.base_domain,parsed_uri.path)\n",
    "                            elif parsed_uri.netloc==self.base_domain and re.match('^http', parsed_uri.scheme):\n",
    "                                absolute_url=url\n",
    "                            elif parsed_uri.netloc==self.base_domain and parsed_uri.scheme==\"\" and re.match(r'^\\/.*\\w$', parsed_uri.path):\n",
    "                                absolute_url=self.scheme+'://'+parsed_uri.netloc+parsed_uri.path\n",
    "                            else:\n",
    "                                continue    \n",
    "                            if absolute_url!='' and absolute_url not in urls:   \n",
    "                                 urls[absolute_url]=0\n",
    "                  \n",
    "                    self.SaveToCsv_FromResponse(url, response)  \n",
    "              \n",
    "                if pages_visited>self.Max_Pages:\n",
    "                    break \n",
    "             \n",
    "                unvisited=[name for name, code in urls.items() if code==0]    \n",
    "                #sorting rule: has keywords, short url, else\n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*100+10)/len(item))\n",
    "                time.sleep(self.Gap_Insecond) #wait for few seconds. \n",
    "          \n",
    "        except: \n",
    "            print('url \"{}\" went wrong'.format(url))  \n",
    "            urls[url]=999\n",
    "                #not to consider failed pages. status_code 400s may need manual handling of they are high priority pages\n",
    "        finally: \n",
    "            self.allurls=urls\n",
    "            #csv_columns = ['url', 'status_code']  \n",
    "            #try:\n",
    "                #with open(self.existingurlfile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    #writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                    #writer.writeheader()\n",
    "                    #for data in urls:\n",
    "                        #writer.writerow(data)\n",
    "            #except IOError:\n",
    "                #print(\"I/O error\")\n",
    "            self.Save_Summaries()\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "        display summaries \n",
    "    \"\"\"\n",
    "    \n",
    "    def Save_Summaries(self): \n",
    "        if self.allurls:\n",
    "            df_urls=pd.DataFrame(list(c.allurls.items()), columns=['url', 'status_code'])\n",
    "            print('Summary for college ', self.college)\n",
    "            print('\\n')\n",
    "            print(df_urls.groupby('status_code').count().reset_index())\n",
    "            #save file as well  \n",
    "            try:\n",
    "                with open(self.existingurlfile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "                    writer.writerow(['url','status_code'])\n",
    "                    for url, status_code in self.allurls.items():  \n",
    "                        writer.writerow([url, status_code])  \n",
    "            except IOError:\n",
    "                print('IO Error in saving summaries')\n",
    "                \n",
    "        if self.files:\n",
    "            print('\\nThe following {} file(s) are generated. '.format(len(self.files)))\n",
    "            pprint(self.files) \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def Read_Oneurl(self, url):  \n",
    "        response=requests.get(url, self.headers)  \n",
    "        if response.status_code==200: \n",
    "            return self.Get_Pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "    \n",
    "    '''\n",
    "        used for filter()\n",
    "    '''\n",
    "    def Tag_Visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "\n",
    "    '''\n",
    "        get all text from page body\n",
    "    '''\n",
    "    def Get_Pagetext(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #conent\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.Tag_Visible, texts)       \n",
    "\n",
    "        return [ [t.parent.name,   \n",
    "                 t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "                 t.nextSibling.name if t.nextSibling!=None else None,\n",
    "                 re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "\n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "        filename, if not None, should not be full name. use import ntpath ntpath.basename(\"a\\b\\c\")\n",
    "    \"\"\"\n",
    "    def SaveToCsv_FromUrl(self,url): # tab delimiter only \n",
    "        url=url.strip() \n",
    "        content=self.Read_Oneurl(url) #in format of (a,a,a,a)   \n",
    "        self.SaveToCsv(url, content)\n",
    "        #return SaveToCsv_FromResponse()\n",
    "    \n",
    "    '''\n",
    "        save from resonse. called in the initial loop\n",
    "    '''\n",
    "    def SaveToCsv_FromResponse(self, url, response):\n",
    "        content= self.Get_Pagetext(response)\n",
    "        self.SaveToCsv(url, content) \n",
    "        \n",
    "    '''\n",
    "        save to csv file and append it to self.files\n",
    "    '''    \n",
    "    def SaveToCsv(self, url, content):\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "        fullname=path.join(self.save_to_folder, filename)\n",
    "        try:\n",
    "            with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "                for lll in content: \n",
    "                    lll.insert(0, url)\n",
    "                    writer.writerow(lll)  \n",
    "            self.files.append(fullname) \n",
    "        except IOError:\n",
    "            print('failed to save file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T16:14:29.254563Z",
     "start_time": "2019-12-29T16:14:29.251560Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T16:14:43.982063Z",
     "start_time": "2019-12-29T16:14:43.977063Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T16:03:01.658686Z",
     "start_time": "2019-12-29T16:03:01.655685Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
