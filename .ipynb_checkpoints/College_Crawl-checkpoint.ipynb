{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollegeCrawl(object):\n",
    "    gap_Insecond=5\n",
    "    max_Pages=150\n",
    "    allUrls=[]\n",
    "    visitedUrls=[]\n",
    "    rejectedUrls=[]\n",
    "    n_allUrls=0\n",
    "    n_sitedUrls=0\n",
    "    n_rejectedUrls=0\n",
    "    \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: www.university.edu\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then everth page \n",
    "        respectrobottxt: True\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, _prioritykeywords, _respectrobottxt=True):\n",
    "        self.college=_collegename\n",
    "        self.rootUrl=_rooturl\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def all_pages(self, base_url, links_only=True):\n",
    "            response=requests.get(base_url)\n",
    "            allUrls={base_url}\n",
    "            visitedUrls=set()\n",
    "            rejectedUrls=set()\n",
    "            while len(allUrls)>len(visitedUrls):\n",
    "                soup=BeautifulSoup(response.text, 'html.parser')\n",
    "                for link in soup.find_all('a'):\n",
    "                    try:\n",
    "                        url=link['href']                \n",
    "                        parsed_uri = urlparse(url )\n",
    "                        if parsed_uri.netloc=='':\n",
    "                            absolute_url=base_url+url    \n",
    "                        elif parsed_uri.netloc==base_domain:\n",
    "                            absolute_url=url\n",
    "                        else:\n",
    "                            continue\n",
    "                        clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url).replace('.edu','.edu/').replace('.edu//','.edu/')\n",
    "                        allUrls.add(clean)               \n",
    "                    except:\n",
    "                        continue\n",
    "                unvisited_url=(allUrls-visitedUrls-rejectedUrls).pop()\n",
    "                visitedUrls.add(unvisited_url)\n",
    "                response=requests.get(unvisited_url)\n",
    "                if response.status_code!=200:\n",
    "                    rejectedUrls.add(unvisited_url)  \n",
    "            return [allUrls[0:max_Pages],visitedUrls,rejectedUrls] if len(allUrls)>150 else [allUrls,visitedUrls,rejectedUrls]\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def read_OneUrl(self, url):\n",
    "        response=requests.get(url)  \n",
    "        if response.status_code==200: \n",
    "            return self.get_pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "            \n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "\n",
    "    def get_pagetext(self, body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "        \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    \n",
    "    def save_OnePage(self,url,folder=None, filename=None, format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "        if folder==None:\n",
    "            folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder doesnot exist')\n",
    "            return\n",
    "        if filename==None:\n",
    "            filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
    "        return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def Save_Summaries(self, urls):\n",
    "        print('There were ',len(list(urls[1]))+len(list(urls[2])),' total urls in ',self.rootUrl)\n",
    "        print('There were ',len(list(urls[1])),' visited urls in ',self.rootUrl)\n",
    "        print('There were ',len(list(urls[2])),' rejected urls in ',self.rootUrl)\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['UCLA','Yale','University of Washington','Stanford']\n",
    "urls=['http://www.ucla.edu/','http://yale.edu/','http://washington.edu/','http://stanford.edu/']\n",
    "k=['apply,admission']\n",
    "for q in range(4):\n",
    "    c=CollegeCrawl(names[q],urls[q],k)\n",
    "    a=c.all_pages(urls[q])\n",
    "    for x in list(a[1]):\n",
    "        c.save_OnePage(x)\n",
    "    c.Save_Summaries(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
